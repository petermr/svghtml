<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 1.1?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">5676666</article-id><article-id pub-id-type="doi">10.3390/s17102262</article-id><article-id pub-id-type="publisher-id">sensors-17-02262</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A Method for Automatic Surface Inspection Using a Model-Based 3D Descriptor</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-4775-9642</contrib-id><name><surname>Madrigal</surname><given-names>Carlos A.</given-names></name><xref ref-type="aff" rid="af1-sensors-17-02262">1</xref><xref rid="c1-sensors-17-02262" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Branch</surname><given-names>John W.</given-names></name><xref ref-type="aff" rid="af2-sensors-17-02262">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-8978-2077</contrib-id><name><surname>Restrepo</surname><given-names>Alejandro</given-names></name><xref ref-type="aff" rid="af2-sensors-17-02262">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-4748-3882</contrib-id><name><surname>Mery</surname><given-names>Domingo</given-names></name><xref ref-type="aff" rid="af3-sensors-17-02262">3</xref></contrib></contrib-group><aff id="af1-sensors-17-02262"><label>1</label>Departamento de Ingenier&#x000ed;a Electr&#x000f3;nica, Instituto Tecnol&#x000f3;gico Metropolitano, Medell&#x000ed;n 050013, Colombia</aff><aff id="af2-sensors-17-02262"><label>2</label>Facultad de Minas, Universidad Nacional de Colombia, Medell&#x000ed;n 050041, Colombia; <email>jwbranch@unal.edu.co</email> (J.W.B.); <email>arestre5@unal.edu.co</email> (A.R.)</aff><aff id="af3-sensors-17-02262"><label>3</label>Departamento de Ciencias de la Computaci&#x000f3;n, Pontificia Universidad Cat&#x000f3;lica de Chile, Santiago 7820436, Chile; <email>domingo.mery@uc.cl</email></aff><author-notes><corresp id="c1-sensors-17-02262"><label>*</label>Correspondence: <email>carlosmadrigal@itm.edu.co</email>; Tel.: +57-4-460-0727</corresp></author-notes><pub-date pub-type="epub"><day>02</day><month>10</month><year>2017</year></pub-date><pub-date pub-type="collection"><month>10</month><year>2017</year></pub-date><volume>17</volume><issue>10</issue><elocation-id>2262</elocation-id><history><date date-type="received"><day>14</day><month>8</month><year>2017</year></date><date date-type="accepted"><day>20</day><month>9</month><year>2017</year></date></history><permissions><copyright-statement>&#x000a9; 2017 by the authors.</copyright-statement><copyright-year>2017</copyright-year><license license-type="open-access"><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Automatic visual inspection allows for the identification of surface defects in manufactured parts. Nevertheless, when defects are on a sub-millimeter scale, detection and recognition are a challenge. This is particularly true when the defect generates topological deformations that are not shown with strong contrast in the 2D image. In this paper, we present a method for recognizing surface defects in 3D point clouds. Firstly, we propose a novel 3D local descriptor called the Model Point Feature Histogram (MPFH) for defect detection. Our descriptor is inspired from earlier descriptors such as the Point Feature Histogram (PFH). To construct the MPFH descriptor, the models that best fit the local surface and their normal vectors are estimated. For each surface model, its contribution weight to the formation of the surface region is calculated and from the relative difference between models of the same region a histogram is generated representing the underlying surface changes. Secondly, through a classification stage, the points on the surface are labeled according to five types of primitives and the defect is detected. Thirdly, the connected components of primitives are projected to a plane, forming a 2D image. Finally, 2D geometrical features are extracted and by a support vector machine, the defects are recognized. The database used is composed of 3D simulated surfaces and 3D reconstructions of defects in welding, artificial teeth, indentations in materials, ceramics and 3D models of defects. The quantitative and qualitative results showed that the proposed method of description is robust to noise and the scale factor, and it is sufficiently discriminative for detecting some surface defects. The performance evaluation of the proposed method was performed for a classification task of the 3D point cloud in primitives, reporting an accuracy of 95%, which is higher than for other state-of-art descriptors. The rate of recognition of defects was close to 94%.</p></abstract><kwd-group><kwd>3D point cloud</kwd><kwd>3D inspection</kwd><kwd>surface quality inspection</kwd><kwd>defects detection</kwd></kwd-group></article-meta></front><body><sec id="sec1-sensors-17-02262"><title>1. Introduction</title><p>Industrial inspection has to date been an exclusively human task. However, maintaining a group of people detecting surface defects accelerates visual fatigue and misinterpretations. Automatic visual inspection allows for detection of defects by image analysis. This process improves product quality, increases the production rate, avoids errors caused by subjectivity, integrates other systems into the production line, and reduces costs [<xref rid="B1-sensors-17-02262" ref-type="bibr">1</xref>,<xref rid="B2-sensors-17-02262" ref-type="bibr">2</xref>,<xref rid="B3-sensors-17-02262" ref-type="bibr">3</xref>]. Industrial machine vision applications are classified into four quality categories according to the characteristics of the inspected product. These categories refer to dimensional, structural or proper assembly, superficial, and operational features [<xref rid="B4-sensors-17-02262" ref-type="bibr">4</xref>]. An estimated 10% of failures in the manufactured parts are caused by superficial defects [<xref rid="B5-sensors-17-02262" ref-type="bibr">5</xref>]. The&#x000a0;surface quality inspection searches for holes, scratches, cracks, wear, finish, roughness, texture, joints, folds, discontinuities, etc. [<xref rid="B6-sensors-17-02262" ref-type="bibr">6</xref>].</p><p>Approaches to surface defect detection are summarized in two main research directions. The&#x000a0;first approach uses complex lighting systems to generate contrast changes in the surface defect [<xref rid="B7-sensors-17-02262" ref-type="bibr">7</xref>]. These methods detect and recognize the defect from the measurement and characterization of pattern deformation on the surface of the object. Zhi and Johansson [<xref rid="B8-sensors-17-02262" ref-type="bibr">8</xref>] presented a method for interpreting the deformation of the fringes based on 14 shape features without measuring the depth. Caulier et al. [<xref rid="B9-sensors-17-02262" ref-type="bibr">9</xref>] proposed a set of eight features suited to the problem of surface inspection, combined with six characteristics that were developed for the classification of defects in interferometry images. Osten et al. [<xref rid="B10-sensors-17-02262" ref-type="bibr">10</xref>] carried out a study of the behavior of the fringes in different defects with the aim of developing inspection systems based on knowledge. Li et al. [<xref rid="B11-sensors-17-02262" ref-type="bibr">11</xref>] proposed an automatic inspection scheme for fabric defect detection using smart visual sensors. The&#x000a0;system consisted of multiple smart visual sensors working independently. Mart&#x000ed;nez et al. [<xref rid="B12-sensors-17-02262" ref-type="bibr">12</xref>] presented a machine vision system, with an easily configurable hardware-software structure, for&#x000a0;surface quality inspection of transparent parts. Neogi et al. [<xref rid="B6-sensors-17-02262" ref-type="bibr">6</xref>] presented a detailed review of vision-based steel surface inspection systems. Also, a comparison was made by typology of the defect, extracted features, and detection accuracy between different defect detection systems. In [<xref rid="B13-sensors-17-02262" ref-type="bibr">13</xref>] a machine vision system able to achieve fused image acquirement and defect inspection for the textured surface with a suitable efficiency and accuracy was presented. The second direction of investigation focuses on the 3D surface reconstruction of the object, and from the 3D point cloud features are extracted to detect and classify the defect. Pernkof [<xref rid="B14-sensors-17-02262" ref-type="bibr">14</xref>] proposed an approach for the 3D reconstruction of raw steel blocks in industrial environments using light sectioning. Ogun et al. [<xref rid="B15-sensors-17-02262" ref-type="bibr">15</xref>] proposed a method for identifying conical defects on simple surfaces and measuring its volume automatically from the 3D reconstruction of the part. Chu and Wang [<xref rid="B16-sensors-17-02262" ref-type="bibr">16</xref>] presented a automated vision-based system for measure the weld bead size and detect defects. Song et al. [<xref rid="B17-sensors-17-02262" ref-type="bibr">17</xref>] proposed a method for fabric defect identification in the textile industry using three-dimensional color phase shift profilometry. A procedure to extract fundamental quality parameters to assess the quality of welds was proposed in [<xref rid="B18-sensors-17-02262" ref-type="bibr">18</xref>]. They used a structured light system to obtain 3D reconstruction.</p><p>The works that interpret the deformation of the illumination on the defects in the 2D image present some limitations. The 2D image does not provide enough information to enable recognition between different typologies of defects. When there are variations in the scale and geometry of defects, determining the appropriate lighting system is a challenge. The geometry and texture of the object generate brightness and shadows that may be confused with a defect. From the 2D image, it is not possible to perform precise metrology of the defect. In&#x000a0;the literature some approaches have been reported to detect and recognize defects from the 3D reconstruction of the piece. However, this is in specific domains of an application where the geometries of surfaces have few variations and only one type of defect is recognized. Because of the above, the&#x000a0;methods of describing the defect are simple, which&#x000a0;limits them to recognition of a set of larger defects.</p><p>The problem of recognizing defects in a 3D surface is similar to the task of recognizing objects in 3D images from partial views. This is not a simple problem, taking into account different variations that the object/surface may suffer, such as translation, rotation, scaling, noise addition, lack&#x000a0;of information, and in some cases, non-rigid deformations. For this reason it is necessary to create robust descriptors for different variations suffered by the defect and the surface. In&#x000a0; [<xref rid="B19-sensors-17-02262" ref-type="bibr">19</xref>] a method for the extraction of characteristics is proposed, called Point Signature, which&#x000a0;creates a signature of each point using the intersection with the surface of a sphere centered at the point. A disadvantage of this method lies in the calculation of what the author called signature point, as the intersection of the sphere with the surface is not easy to calculate and in some cases it is necessary to interpolate points, which&#x000a0;leads to a reduced accuracy of the signature point and increases the computational cost. In&#x000a0;addition, the&#x000a0;calculation of the reference vector is sensitive to noise.</p><p>Johnson and Hebert [<xref rid="B20-sensors-17-02262" ref-type="bibr">20</xref>] proposed Spin Image, which&#x000a0;is a point descriptor based on the projection of the adjacent 3D points on a tangential 2D plane. It obtains a 2D image for that point, which becomes the characteristic. A disadvantage of Spin Image is its high dependence on the resolution of the method. Some modifications have been proposed for interpolation, increasing the computational cost. In&#x000a0; Reference [<xref rid="B21-sensors-17-02262" ref-type="bibr">21</xref>] a method called the Point Feature Histogram (PFH) is proposed to describe the surface of the neighborhood of a point by the difference between the normals. Although an advantage of the method is the simplicity of its calculation, a greater disadvantage lies in the construction of the histogram for each point from the calculation of the differences between normals of an entire region. Full connectivity is created, which&#x000a0;generates a smoothing effect for small changes in the region. This also affects the processing times. In Reference [<xref rid="B22-sensors-17-02262" ref-type="bibr">22</xref>] a modification to the PFH method improving the processing time was presented, called the Fast Point Feature Histogram (FPFH). It does not create full connectivity for the construction of the histogram, at the cost of reducing the discriminant ability of the method. In Reference [<xref rid="B23-sensors-17-02262" ref-type="bibr">23</xref>] the Viewpoint Feature Histogram (VFH) is presented, which&#x000a0;is a modification to the FPFH method to describe an object globally by coding its geometry and point of&#x000a0;view.</p><p>In the recognition of surface defects of objects on a submillimeter scale, the&#x000a0;acquisition conditions strongly affect the recognition process. Moreover, it is considered that defects are composed of abrupt changes in the surface, which&#x000a0;generate glare and shadows in the 3D reconstruction process and cause low point density in these defective regions. Therefore, the&#x000a0;descriptors used to represent the underlying surface must be sufficiently stable to conditions of low point density and noise in the data. The&#x000a0;PFH, FPFH and Spin Image, by including the normals in the calculation of the descriptor, are&#x000a0;representation methods that highly depend on the correct estimation of the normals and their support radius, which&#x000a0;is difficult to ensure in the presence of a defect. The&#x000a0;PFH uses the difference between all the normals in the vicinity of a keypoint. As such, in regions with small changes the normals of these points are averaged with the normals of regions with few changes, hence causing a loss of information when describing the defect region.</p><p>This paper proposes a descriptor that allows discrimination of different regions within a 3D surface. Our descriptor is based on the estimation of local models on the surface and the difference between the model&#x02019;s normals in a local region.</p><p>The contributions to this paper can be summarized as follows:<list list-type="bullet"><list-item><p>The new local 3D surface descriptor the Model Point Feature Histogram (MPFH) is presented, improving robustness and discriminant capabilities of the PFH method.The MPFH descriptor is constructed from the estimation of models and their weighting to the formation of the underlying&#x000a0;region.</p></list-item><list-item><p>Furthermore, a methodology for the automatic surface defects inspection is presented. For&#x000a0;the detection of the defect, the&#x000a0;local 3D descriptor MPFH is used in 3D reconstructed objects. From&#x000a0;the projection 2D of the detected 3D primitives are extracted 2D features that allow to classification of the&#x000a0;defect.</p></list-item></list></p><p>This paper is organized as follows: <xref ref-type="sec" rid="sec2-sensors-17-02262">Section 2</xref> provides an overview of the methodology. It&#x000a0;emphasizes the calculation of structures, estimation of the normals, and the proposed descriptor. <xref ref-type="sec" rid="sec3-sensors-17-02262">Section 3</xref> describes the experimental results and, finally, the&#x000a0;conclusions are drawn in <xref ref-type="sec" rid="sec4-sensors-17-02262">Section 4</xref>.</p></sec><sec id="sec2-sensors-17-02262"><title>2. System Overview</title><p><xref ref-type="fig" rid="sensors-17-02262-f001">Figure 1</xref> illustrates the scheme of the methodology proposed to recognize surface defects. Our&#x000a0;methodology begins with surface 3D reconstruction by projecting structured light patterns (<xref ref-type="fig" rid="sensors-17-02262-f001">Figure 1</xref>a). As a result, a 3D point cloud is obtained which represents the sampled 3D surface (<xref ref-type="fig" rid="sensors-17-02262-f001">Figure 1</xref>b), followed by the calculation of the models or planes that fit each region best using the multiple structures estimation method J-linkage [<xref rid="B24-sensors-17-02262" ref-type="bibr">24</xref>] (<xref ref-type="fig" rid="sensors-17-02262-f001">Figure 1</xref>c), and&#x000a0;then the normals of the surface models are estimated (<xref ref-type="fig" rid="sensors-17-02262-f001">Figure 1</xref>d). For&#x000a0;each surface model, the&#x000a0;model contribution weight to the formation of the surface region (<xref ref-type="fig" rid="sensors-17-02262-f001">Figure 1</xref>e) is calculated and from the relative difference between two models of the same region a histogram is generated representing the underlying surface changes (<xref ref-type="fig" rid="sensors-17-02262-f001">Figure 1</xref>f). Each point on the 3D surface is classified into one of five types of primitives, points belonging to hollow, hollow edge, crest, base crest, or flat surfaces (<xref ref-type="fig" rid="sensors-17-02262-f001">Figure 1</xref>g). In&#x000a0;this work, it is considered that with this set of primitives it is possible to describe any typology of the defect. For&#x000a0;instance, a bump defect will be composed of the base crest, crest, and&#x000a0;possibly flat primitives; therefore regions with local geometric changes are detected on the surface. Finally, these regions are extracted 2D features to recognize the defect in a classification stage (<xref ref-type="fig" rid="sensors-17-02262-f001">Figure 1</xref>h).</p><sec id="sec2dot1-sensors-17-02262"><title>2.1. Image Acquisition</title><p>The 3D point cloud is acquired from a 3D reconstruction system. The&#x000a0;technique of projecting structured light patterns was used. Specifically, sinusoidal phase shift patterns were projected. A complementary metal-oxide semiconductor (CMOS) camera (Point Grey FL3-U3-88S2C-C, Richmond, BC, Canada) was used to capture the images and a digital light projector (DLP) was used to synchronously project the sinusoidal patterns on the object [<xref rid="B25-sensors-17-02262" ref-type="bibr">25</xref>]. The&#x000a0;acquisition scheme is shown in <xref ref-type="fig" rid="sensors-17-02262-f002">Figure 2</xref>. The&#x000a0;specifications of the camera and the projector are shown in <xref ref-type="table" rid="sensors-17-02262-t001">Table 1</xref>. Our 3D reconstruction system had an RMS measurement error of 0.053 mm in <italic>x</italic>, 0.039 mm in <italic>y</italic> and 0.10 mm in <italic>z</italic>, for&#x000a0;a measurement area of 60 mm &#x000d7; 60 mm.</p></sec><sec id="sec2dot2-sensors-17-02262"><title>2.2. Calculation of Structures</title><p>Superficial defects generate deformations on the object&#x02019;s surface; this causes the normals and curvature in a defective region to change with respect to the neighborhood region. Therefore,&#x000a0;if&#x000a0;the representation method includes the normal or curvature in the construction of the descriptor, its&#x000a0;adequate estimation influences the description of the region. Given a point cloud of the sample 3D surface called <italic>S</italic>, let <inline-formula><mml:math id="mm1"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> be point on the surface; the <italic>k</italic> closest adjacent points around <inline-formula><mml:math id="mm2"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are denoted by <inline-formula><mml:math id="mm3"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The&#x000a0;problem of choosing <italic>k</italic> is known as the correct scale factor and it affects the estimation of the normal vectors. The parameter <italic>k</italic> is also related to the minimum defect size that our system is capable of detecting. To estimate the normal to a point <inline-formula><mml:math id="mm4"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> on the surface, a least squares local plane is fitted to <inline-formula><mml:math id="mm5"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and its normal vector becomes normal to the point <inline-formula><mml:math id="mm6"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> [<xref rid="B26-sensors-17-02262" ref-type="bibr">26</xref>]. However, the&#x000a0;normals of the planes that are adjusted to points <inline-formula><mml:math id="mm7"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, with very big <italic>k</italic> or belonging to edges or corners, undergo a smoothing effect leading to misdirection, which&#x000a0;influences the description of that region. The&#x000a0;normals shown in red in <xref ref-type="fig" rid="sensors-17-02262-f003">Figure 3</xref>a present these effects.</p><p>A structure is defined as a set of points that adjusts to a plane. We call these structures &#x0201c;models&#x0201d; as they represent the surface changes in a simple manner. The&#x000a0;region <inline-formula><mml:math id="mm8"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> around the corner is composed of three structures or models with different normal vectors (<xref ref-type="fig" rid="sensors-17-02262-f003">Figure 3</xref>a). Thus, our methodology proposes using a method to estimate the multiple structures <inline-formula><mml:math id="mm9"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> in a region <inline-formula><mml:math id="mm10"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and then calculate their normal vectors. A model is denoted by <inline-formula><mml:math id="mm11"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mfenced separators="" open="{" close="}"><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>&#x003c4;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c4;</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Moreover, it associates the normal vector of the model <inline-formula><mml:math id="mm12"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to each point <inline-formula><mml:math id="mm13"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, which&#x000a0;makes it not strictly necessary to calculate the normals for each point of the surface, but rather ensures that the joining of the points that belong to the generated models is equal to the surface <italic>S</italic>. This is shown in Equation (<xref ref-type="disp-formula" rid="FD1-sensors-17-02262">1</xref>), where <italic>n</italic> represents the total number of models found on the surface. <xref ref-type="fig" rid="sensors-17-02262-f003">Figure 3</xref>b shows the normal vectors on the surface using the proposed methodology, where is possible observe the correct directing of the normals for points belonging to edges and corners. The&#x000a0;number of <inline-formula><mml:math id="mm14"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> models in a region <inline-formula><mml:math id="mm15"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> depends on its topology, so that if the region has few variations, the&#x000a0;number of found models is low compared to a region presenting many topological changes. <xref ref-type="fig" rid="sensors-17-02262-f003">Figure 3</xref>c represents this concept, where the red dots indicate the center of each estimated model on a defective surface.
<disp-formula id="FD1-sensors-17-02262"><label>(1)</label><mml:math id="mm16"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x022c3;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msub><mml:mi>M</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The nature of the problem described above is comprised of a set of data that belongs to multiple structures and outliers. The&#x000a0;challenge is to estimate the different structures in the presence of noise and geometric changes in the point cloud.</p><p>We use the approach proposed in [<xref rid="B24-sensors-17-02262" ref-type="bibr">24</xref>], the&#x000a0;J-linkage method, to estimate the multiple structures on a local region of points <inline-formula><mml:math id="mm17"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, which&#x000a0;is composed of the <italic>k</italic>-nearest neighbors of a point <inline-formula><mml:math id="mm18"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. As in Random Sample Consensus (RANSAC) [<xref rid="B27-sensors-17-02262" ref-type="bibr">27</xref>], J-linkage generates a random set of <italic>M</italic> hypothesis models called a minimum sampling set. Then the consensus set (CS) of each model is determined as the set of points that is at a smaller distance than a threshold <inline-formula><mml:math id="mm19"><mml:mrow><mml:mi>&#x003f5;</mml:mi></mml:mrow></mml:math></inline-formula> for this particular model. From the CS of the <italic>M</italic> hypothesis models, a <inline-formula><mml:math id="mm20"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> vector is built, representing the set of models to which <inline-formula><mml:math id="mm21"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> belongs, where 1 in the column <italic>j</italic> of the vector represents the belonging of that point to the <inline-formula><mml:math id="mm22"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> model and 0 represents that the point does not belonging to the model. This vector is called the preference set (PS) of a point <inline-formula><mml:math id="mm23"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, which&#x000a0;becomes a conceptual representation of this point, so that points within the same structure will have similar representations. They will be grouped together as in [<xref rid="B24-sensors-17-02262" ref-type="bibr">24</xref>], in a conceptual space <inline-formula><mml:math id="mm24"><mml:mrow><mml:msup><mml:mfenced separators="" open="{" close="}"><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mfenced><mml:mi>M</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. To measure this similarity, the Jaccard distance is used (Equation&#x000a0;(<xref ref-type="disp-formula" rid="FD2-sensors-17-02262">2</xref>)). <italic>A</italic> and <italic>B</italic> are two PSs different to &#x02298; and <inline-formula><mml:math id="mm25"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>J</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> takes on values between 0 and 1, for&#x000a0;identical and different sets,&#x000a0;consecutively.
<disp-formula id="FD2-sensors-17-02262"><label>(2)</label><mml:math id="mm26"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>J</mml:mi></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mfenced separators="" open="" close=""><mml:mi>A</mml:mi><mml:mo>&#x0222a;</mml:mo><mml:mi>B</mml:mi></mml:mfenced><mml:mo>&#x02212;</mml:mo><mml:mfenced separators="" open="" close=""><mml:mi>A</mml:mi><mml:mo>&#x02229;</mml:mo><mml:mi>B</mml:mi></mml:mfenced></mml:mrow><mml:mfenced separators="" open="" close=""><mml:mi>A</mml:mi><mml:mo>&#x0222a;</mml:mo><mml:mi>B</mml:mi></mml:mfenced></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>J-linkage has been used to determine the multiple flat structures in a scene for segmentation purposes from point clouds [<xref rid="B28-sensors-17-02262" ref-type="bibr">28</xref>,<xref rid="B29-sensors-17-02262" ref-type="bibr">29</xref>], where the difference between the functions of the flat structures is high and the amount of structures is low compared to the density of points. This difference does not arise with point clouds belonging to 3D reconstructions of surface defects, where noise can easily be an inlier of structures and generate a very large number of structures. Therefore, we first generated local regions <inline-formula><mml:math id="mm27"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> throughout the point cloud, and&#x000a0;then the models of each <inline-formula><mml:math id="mm28"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> region were determined using J-linkage. As a result, the estimated models represent local changes in the surface.</p><p>A model of a plane can be represented from up to three points. However, we found experimentally that accepting models with fewer than five points increases false models due the noisy data. Increasing the number of points results in that in defective regions, where the point density is low, the&#x000a0;models are&#x000a0;rejected.</p></sec><sec id="sec2dot3-sensors-17-02262"><title>2.3. Normal Estimation</title><p>After obtaining the <italic>M</italic> models belonging to a neighborhood <inline-formula><mml:math id="mm29"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of <inline-formula><mml:math id="mm30"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the&#x000a0;normal vector to the model is determined from the analysis of the eigenvalues <inline-formula><mml:math id="mm31"><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow></mml:math></inline-formula> and eigenvectors <inline-formula><mml:math id="mm32"><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> of the covariance matrix <italic>C</italic>, as described in Equation&#x000a0;(<xref ref-type="disp-formula" rid="FD3-sensors-17-02262">3</xref>).
<disp-formula id="FD3-sensors-17-02262"><label>(3)</label><mml:math id="mm33"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>&#x003c4;</mml:mi></mml:mfrac><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mover><mml:mi>P</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mover><mml:mi>P</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The eigenvector <inline-formula><mml:math id="mm34"><mml:mrow><mml:mover><mml:msub><mml:mi>v</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>&#x02192;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> of the smallest eigenvalue <inline-formula><mml:math id="mm35"><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> corresponds to the approximation of the normal <inline-formula><mml:math id="mm36"><mml:mrow><mml:mover><mml:mi>N</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, if the condition <inline-formula><mml:math id="mm37"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#x02265;</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x02265;</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>&#x02265;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> is fulfilled [<xref rid="B30-sensors-17-02262" ref-type="bibr">30</xref>].</p><p>Using models to determine the normal vectors to a surface of a region solves to a large extent the problem of the right scale factor when calculating the normals (see <xref ref-type="sec" rid="sec3dot2-sensors-17-02262">Section 3.2</xref>). The&#x000a0;problem of the scale factor is associated with the selected support radii to estimate the normals. If the scale factor (<italic>k</italic> or <italic>r</italic> search) is big, the&#x000a0;region will undergo a smoothing effect and the normal vector will not capture its details. Algorithm&#x000a0;1 shows the pseudo-code for determining the models and their associated normal vectors and <xref ref-type="fig" rid="sensors-17-02262-f004">Figure 4</xref> shows the result of the estimation of the normal vectors on a surface sampled containing a hole defect.</p><array orientation="portrait"><tbody><tr><td style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 1:</bold> Normals estimation</td></tr><tr><td style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-graphic xlink:href="sensors-17-02262-i001.jpg"/></td></tr></tbody></array></sec><sec id="sec2dot4-sensors-17-02262"><title>2.4. Model Points Feature Histogram (MPFH)</title><p>After obtaining the models and its normal vectors, it is necessary to calculate a representation for each point <inline-formula><mml:math id="mm70"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of the surface sampled <italic>S</italic>. For this, we propose calculating a histogram of characteristics similar to that proposed in [<xref rid="B31-sensors-17-02262" ref-type="bibr">31</xref>].</p><p>The calculated feature is the relative difference between the normal vectors of each two models belonging to the region <inline-formula><mml:math id="mm71"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> in a Darboux coordinate system. This coordinate system generates a 4D feature that is invariant to the translation and rotation, which has the advantage of reducing the number of parameters from 12 to 4 parameters. This reduction is composed of the coordinates <inline-formula><mml:math id="mm72"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> and normal vectors <inline-formula><mml:math id="mm73"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">N</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> of the two points.</p><p>The pair <inline-formula><mml:math id="mm74"><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">N</mml:mi></mml:mfenced></mml:mrow></mml:math></inline-formula> is known as surflet. The origin will be <inline-formula><mml:math id="mm75"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mn mathvariant="bold">1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, if Equation (<xref ref-type="disp-formula" rid="FD4-sensors-17-02262">4</xref>) is met, otherwise it will be <inline-formula><mml:math id="mm76"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mn mathvariant="bold">2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and the indexes 1 y 2 in the equations must be exchanged. This is to achieve homogeneity in the choice of the origin of the coordinate system.</p><p>Let &#x000b7; denote the scalar product between two vectors and &#x000d7; the cross product of the two vectors.
<disp-formula id="FD4-sensors-17-02262"><label>(4)</label><mml:math id="mm77"><mml:mrow><mml:mrow><mml:mfenced separators="" open="|" close="|"><mml:msub><mml:mi mathvariant="bold-italic">N</mml:mi><mml:mn mathvariant="bold">1</mml:mn></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mn mathvariant="bold">2</mml:mn></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mn mathvariant="bold">1</mml:mn></mml:msub></mml:mfenced></mml:mfenced><mml:mo>&#x02264;</mml:mo><mml:mfenced separators="" open="|" close="|"><mml:msub><mml:mi mathvariant="bold-italic">N</mml:mi><mml:mn mathvariant="bold">2</mml:mn></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mn mathvariant="bold">2</mml:mn></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mn mathvariant="bold">1</mml:mn></mml:msub></mml:mfenced></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The axes of the coordinate system are expressed in Equations (<xref ref-type="disp-formula" rid="FD5-sensors-17-02262">5</xref>)&#x02013;(<xref ref-type="disp-formula" rid="FD7-sensors-17-02262">7</xref>), and the features indicating the differences between two surflets are described in Equations (<xref ref-type="disp-formula" rid="FD8-sensors-17-02262">8</xref>)&#x02013;(<xref ref-type="disp-formula" rid="FD11-sensors-17-02262">11</xref>), [<xref rid="B31-sensors-17-02262" ref-type="bibr">31</xref>]. <xref ref-type="fig" rid="sensors-17-02262-f005">Figure 5</xref> shows the relationship between the coordinate systems and the features obtained.</p><disp-formula id="FD5-sensors-17-02262"><label>(5)</label><mml:math id="mm78"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">N</mml:mi><mml:mn mathvariant="bold">1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD6-sensors-17-02262"><label>(6)</label><mml:math id="mm79"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mn mathvariant="bold">2</mml:mn></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mn mathvariant="bold">1</mml:mn></mml:msub></mml:mfenced><mml:mo>&#x000d7;</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mfenced separators="" open="&#x02225;" close="&#x02225;"><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mn mathvariant="bold">2</mml:mn></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mn mathvariant="bold">1</mml:mn></mml:msub></mml:mfenced><mml:mo>&#x000d7;</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mfenced></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD7-sensors-17-02262"><label>(7)</label><mml:math id="mm80"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD8-sensors-17-02262"><label>(8)</label><mml:math id="mm81"><mml:mrow><mml:mrow><mml:mi>&#x003b8;</mml:mi><mml:mo>=</mml:mo><mml:mo form="prefix">arctan</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">N</mml:mi><mml:mn mathvariant="bold">1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">N</mml:mi><mml:mn mathvariant="bold">2</mml:mn></mml:msub></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD9-sensors-17-02262"><label>(9)</label><mml:math id="mm82"><mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">N</mml:mi><mml:mn mathvariant="bold">2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD10-sensors-17-02262"><label>(10)</label><mml:math id="mm83"><mml:mrow><mml:mrow><mml:mi>&#x003d5;</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mfrac><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mn mathvariant="bold">2</mml:mn></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mn mathvariant="bold">1</mml:mn></mml:msub></mml:mfenced><mml:mfenced separators="" open="&#x02225;" close="&#x02225;"><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mn mathvariant="bold">2</mml:mn></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mn mathvariant="bold">1</mml:mn></mml:msub></mml:mfenced></mml:mfenced></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD11-sensors-17-02262"><label>(11)</label><mml:math id="mm84"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mfenced separators="" open="&#x02225;" close="&#x02225;"><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mn mathvariant="bold">2</mml:mn></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mn mathvariant="bold">1</mml:mn></mml:msub></mml:mfenced></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><p>To calculate the descriptor of a point <inline-formula><mml:math id="mm85"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the closest points <inline-formula><mml:math id="mm86"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are searched in a neighborhood <inline-formula><mml:math id="mm87"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> which we call region <inline-formula><mml:math id="mm88"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Then, the models <inline-formula><mml:math id="mm89"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and their centroids to which belong to the region <inline-formula><mml:math id="mm90"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are determined. Hence, a vector with the set of models and another with the level of participation <inline-formula><mml:math id="mm91"><mml:mrow><mml:mi>&#x003c1;</mml:mi></mml:mrow></mml:math></inline-formula> of the model in the representation of the region <inline-formula><mml:math id="mm92"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are computed. <inline-formula><mml:math id="mm93"><mml:mrow><mml:mi>&#x003c1;</mml:mi></mml:mrow></mml:math></inline-formula> represents the ratio between the number of points that form a model <inline-formula><mml:math id="mm94"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and the totality of the points <inline-formula><mml:math id="mm95"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, described in Equation (<xref ref-type="disp-formula" rid="FD12-sensors-17-02262">12</xref>). <inline-formula><mml:math id="mm96"><mml:mrow><mml:mi>&#x003c1;</mml:mi></mml:mrow></mml:math></inline-formula> ranges from <inline-formula><mml:math id="mm97"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>; <inline-formula><mml:math id="mm98"><mml:mrow><mml:mrow><mml:mi>&#x003c1;</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> is not present, because <inline-formula><mml:math id="mm99"><mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02229;</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#x02260;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. <xref ref-type="fig" rid="sensors-17-02262-f005">Figure 5</xref> illustrates the <inline-formula><mml:math id="mm100"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> models belonging to the <inline-formula><mml:math id="mm101"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> region and the difference between them.
<disp-formula id="FD12-sensors-17-02262"><label>(12)</label><mml:math id="mm102"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003c1;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02229;</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The PFH method [<xref rid="B21-sensors-17-02262" ref-type="bibr">21</xref>] uses the histogram of characteristics proposed in [<xref rid="B31-sensors-17-02262" ref-type="bibr">31</xref>], where the features <inline-formula><mml:math id="mm103"><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>&#x003b8;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003d5;</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mfenced></mml:mrow></mml:math></inline-formula> are found between each pair of surflets belonging to the region <inline-formula><mml:math id="mm104"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. It generates a set of <inline-formula><mml:math id="mm105"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:mfrac><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> quadruplets of features with complete connectivity without weighting. As a result, it averages the differences between surflets and smoothes local details of the region. On the other hand, our method is based on the relationship between the normal vectors of the models belonging to the region <inline-formula><mml:math id="mm106"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. In addition, our proposal adds the parameter <inline-formula><mml:math id="mm107"><mml:mrow><mml:mi>&#x003c1;</mml:mi></mml:mrow></mml:math></inline-formula>, which represents the participation of each model to the formation of the region. This speeds up the calculation of the representation and leads to a better capture of the geometric information of the underlying 3D surface. This also reduces the computational complexity.</p><p>To calculate the MPFH representation of each point <inline-formula><mml:math id="mm108"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, a histogram is generated from the quadruplets of the region <inline-formula><mml:math id="mm109"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> using the first three Darboux features and adding the weighting of the model to the formation of the region <inline-formula><mml:math id="mm110"><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>&#x003b8;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003d5;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c1;</mml:mi></mml:mfenced></mml:mrow></mml:math></inline-formula>. Therefore, a histogram <italic>H</italic> with 4 &#x000d7; <italic>b</italic> bins is generated, where the first <italic>b</italic> bins corresponds to the quantization of the feature <inline-formula><mml:math id="mm111"><mml:mrow><mml:mi>&#x003d5;</mml:mi></mml:mrow></mml:math></inline-formula>, the following to <inline-formula><mml:math id="mm112"><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:math></inline-formula>, then to <inline-formula><mml:math id="mm113"><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:math></inline-formula>, and finally the last corresponds to <inline-formula><mml:math id="mm114"><mml:mrow><mml:mi>&#x003c1;</mml:mi></mml:mrow></mml:math></inline-formula>. <xref ref-type="fig" rid="sensors-17-02262-f006">Figure 6</xref>a shows the MPFH representation for a point belonging to a flat region and <xref ref-type="fig" rid="sensors-17-02262-f006">Figure 6</xref>b for a point belonging to a hole-like defect, given <italic>b</italic> equals 11 as in the FPFH method.</p></sec><sec id="sec2dot5-sensors-17-02262"><title>2.5. Primitives 2D Projection</title><p>The identification of primitives allows for detection of defects on surface but not their classification. Our proposal is to project each connected component of primitives on a plane forming a 2D image. Then, 2D geometrical features are extracted for recognition of the defect. <xref ref-type="fig" rid="sensors-17-02262-f007">Figure 7</xref> shows the methodological scheme proposed.</p><p>First, the primitives are clustered in connected components, using Euclidean distance. The threshold distance <inline-formula><mml:math id="mm115"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is selected according to resolution of the point cloud. Then, for each connected component <inline-formula><mml:math id="mm116"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the centroid <inline-formula><mml:math id="mm117"><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is calculated, and with the flat primitive <inline-formula><mml:math id="mm118"><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> nearest to this, the projection plane <inline-formula><mml:math id="mm119"><mml:mrow><mml:msub><mml:mi mathvariant="sans-serif">&#x003a0;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is set. All primitives <inline-formula><mml:math id="mm120"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> are projected orthogonally to the plane <inline-formula><mml:math id="mm121"><mml:mrow><mml:msub><mml:mi mathvariant="sans-serif">&#x003a0;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> as described in Equations (<xref ref-type="disp-formula" rid="FD13-sensors-17-02262">13</xref>)&#x02013;(<xref ref-type="disp-formula" rid="FD15-sensors-17-02262">15</xref>). <xref ref-type="fig" rid="sensors-17-02262-f008">Figure 8</xref>a,b illustrates the procedure.
<disp-formula id="FD13-sensors-17-02262"><label>(13)</label><mml:math id="mm122"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">q</mml:mi><mml:mo>=</mml:mo><mml:mfenced separators="" open="|" close="|"><mml:msub><mml:mi mathvariant="bold-italic">O</mml:mi><mml:mi mathvariant="bold-italic">ci</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">f</mml:mi></mml:msubsup></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD14-sensors-17-02262"><label>(14)</label><mml:math id="mm123"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mo>=</mml:mo><mml:mfenced separators="" open="|" close="|"><mml:mi mathvariant="bold-italic">q</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:msubsup><mml:mi>N</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi></mml:msubsup></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD15-sensors-17-02262"><label>(15)</label><mml:math id="mm124"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mi mathvariant="bold-italic">proj</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">P</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mo>&#x02217;</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">N</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="mm125"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">N</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is the normal vector associated to <inline-formula><mml:math id="mm126"><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>.</p><p>A affine transformation is applied to the plane <inline-formula><mml:math id="mm127"><mml:mrow><mml:msub><mml:mi mathvariant="sans-serif">&#x003a0;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, so that it is parallel to the <inline-formula><mml:math id="mm128"><mml:mrow><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> plane with normal vector <inline-formula><mml:math id="mm129"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>(<inline-formula><mml:math id="mm130"><mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>). Equations (<xref ref-type="disp-formula" rid="FD16-sensors-17-02262">16</xref>)&#x02013;(<xref ref-type="disp-formula" rid="FD18-sensors-17-02262">18</xref>) show the calculate of axis and angle rotation, and Equation (<xref ref-type="disp-formula" rid="FD19-sensors-17-02262">19</xref>) obtains the matrix Euler-Rodriguez formula.
<disp-formula id="FD16-sensors-17-02262"><label>(16)</label><mml:math id="mm131"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">Rot</mml:mi><mml:mi mathvariant="bold-italic">Axis</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">N</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi></mml:msubsup><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD17-sensors-17-02262"><label>(17)</label><mml:math id="mm132"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi mathvariant="bold-italic">Rot</mml:mi><mml:mi mathvariant="bold-italic">Axis</mml:mi></mml:msub><mml:mfenced separators="" open="&#x02225;" close="&#x02225;"><mml:msub><mml:mi mathvariant="bold-italic">Rot</mml:mi><mml:mi mathvariant="bold-italic">Axis</mml:mi></mml:msub></mml:mfenced></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD18-sensors-17-02262"><label>(18)</label><mml:math id="mm133"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">Rot</mml:mi><mml:mi mathvariant="bold-italic">Angle</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">&#x003b8;</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">N</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi></mml:msubsup><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD19-sensors-17-02262"><label>(19)</label><mml:math id="mm134"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mo>=</mml:mo><mml:mo form="prefix">cos</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mo>+</mml:mo><mml:mo form="prefix">sin</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:msub><mml:mfenced separators="" open="[" close="]"><mml:msub><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mfenced><mml:mo>&#x000d7;</mml:mo></mml:msub><mml:mo>+</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mo form="prefix">cos</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mfenced><mml:msub><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02297;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="mm135"><mml:mrow><mml:msub><mml:mfenced separators="" open="[" close="]"><mml:msub><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mfenced><mml:mo>&#x000d7;</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> denotes the cross-product matrix of <inline-formula><mml:math id="mm136"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, &#x02297; is the tensor product, and <inline-formula><mml:math id="mm137"><mml:mrow><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow></mml:math></inline-formula> is the identity matrix. <xref ref-type="fig" rid="sensors-17-02262-f006">Figure 6</xref>b illustrates the result of this procedure.</p><p>The 2D image is formed by the conversion of the plane <inline-formula><mml:math id="mm138"><mml:mrow><mml:msub><mml:mi mathvariant="sans-serif">&#x003a0;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> in mm to pixels. For that, we calculate the factor <inline-formula><mml:math id="mm139"><mml:mrow><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="mm140"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm141"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the closest distance between <inline-formula><mml:math id="mm142"><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> projected at <inline-formula><mml:math id="mm143"><mml:mrow><mml:msub><mml:mi mathvariant="sans-serif">&#x003a0;</mml:mi><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> and other point. Then, the bounding box of <inline-formula><mml:math id="mm144"><mml:mrow><mml:msub><mml:mi mathvariant="sans-serif">&#x003a0;</mml:mi><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> is calculated with the goal of translating it the coordinate origin. <xref ref-type="fig" rid="sensors-17-02262-f008">Figure 8</xref>c shows the 2D image result and <xref ref-type="fig" rid="sensors-17-02262-f008">Figure 8</xref>d,e shows 2D projections of different typologies of defects on the objects of experimentation.</p><p>According to [<xref rid="B32-sensors-17-02262" ref-type="bibr">32</xref>], the characteristics of a surface imperfection are represented in the length, width, depth, height, and area of imperfection. Hence, it is possible to combine 3D and 2D descriptors to recognize an imperfection. As explained in <xref ref-type="sec" rid="sec2dot4-sensors-17-02262">Section 2.4</xref>, the 3D information of the curvature of the surface is embedded in the local 3D descriptor MPFH. However, it is necessary to obtain 2D geometric information that allows recognition of the defect. We proposed to group each set of primitives from its 2D projection and then extract a set of characteristics from each region of primitives belonging to the defect. Finally, in a classification stage the defect region is recognized.</p><p>The set of characteristics was chosen from a process of selection of characteristics using a Fisher discriminant [<xref rid="B33-sensors-17-02262" ref-type="bibr">33</xref>] through the Balu toolbox Matlab [<xref rid="B34-sensors-17-02262" ref-type="bibr">34</xref>]. The most relevant features for each type of primitives were the Hu [H1, H2, H7] moments, the Fourier descriptors [F3, F4, F5, F7 and F11], the eccentricity [Exc], and the number Euler [En].</p></sec></sec><sec id="sec3-sensors-17-02262"><title>3. Results</title><p>In this section, we developed two types of evaluation. First, we proved the discriminative power of the proposed 3D local descriptor compared with five methods of description commonly used. Also, invariance of descriptors to noise and support radius was addressed. Second, we tested the performance of the proposed defects recognition methodology in a database with different objects and defect typology.</p><sec id="sec3dot1-sensors-17-02262"><title>3.1. Database</title><p>According to our review, to date there have not been any public databases of 3D images with surface defects. Therefore, the database used in these experiments was constructed from 3D reconstructions of defects in welding, artificial teeth; indentations in materials, ceramics, models of artificial defects, and simulations. <xref ref-type="fig" rid="sensors-17-02262-f009">Figure 9</xref> shows an example of the objects used in the database. Our database contains 480 three-dimensional images with more than 2000 regions with surface defects. There are four types labeled in the database: holes, bumps, cracks and without defect.</p><p>In order to understand how difficult it is to detect and recognize a micro-metric defect on an object, <xref ref-type="fig" rid="sensors-17-02262-f010">Figure 10</xref>a&#x02013;c shows an indentation with a test of Vickers hardness on a sample of aluminum. In <xref ref-type="fig" rid="sensors-17-02262-f010">Figure 10</xref>d&#x02013;f a material detachment defect in an artificial tooth can be observed.</p></sec><sec id="sec3dot2-sensors-17-02262"><title>3.2. Evaluation of the Estimation of the Normals from Multiple Structures</title><p>In this section, we evaluate how the computation of the normal vector on a surface is affected when simple or multiple structure estimation methodologies are used. In <xref ref-type="fig" rid="sensors-17-02262-f011">Figure 11</xref>, the result of evaluating the change of a normal at a critical point, where abrupt geometrical changes take place, is shown. The number of k-neighbors, which supports the parameter to estimate the models, was varied between 15 and 250 points. As <xref ref-type="fig" rid="sensors-17-02262-f011">Figure 11</xref> shows, the scaling factor does not significantly influence the normal vectors calculated by estimating multiple structures. In contrast to using a unique structure, the normal vector in that region changes with the variation of support radius. This means that through the proposed methodology, better-oriented normal vectors at critical points such as edges and corners are obtained. Consequently, this will increase the discriminating capabilities of our descriptor.</p></sec><sec id="sec3dot3-sensors-17-02262"><title>3.3. Robustness to Noise in the Estimation of the Normal Vectors</title><p>In order to evaluate the robustness to noise in the estimation of models for calculating the normal vectors, in <xref ref-type="table" rid="sensors-17-02262-t002">Table 2</xref> we show a comparison of the results of calculating the normal vectors using the adjustment of multiple- structure J-linkage versus the adjustment of a single structure on a cube. In a simple structure like a cube that has three types of regions (planes, edges, and corners), normal estimation methods based on a single structure fail at edges and corners. The defects, being more complex structures, require greater precision in the estimation of the normal, which can be achieved if the estimation of normal multiples is used. The regions chosen for the evaluation were: points in planes, near edges, and on corners. Gaussian noise with a standard deviation of 5% is also added. The calculation of the normal vectors is compared with the theoretical normal vector. The spaces in the &#x0201c;One Structure&#x0201d; column are due to the fact that, regardless of the topological complexity of a region, the methods based on the estimation of unique structures will always find only one normal vector. However, for example for a cube-type region, there are three theoretical normal vectors which match the results using multiple structures. The mean square error between the calculation of the normal vectors using the estimation of multiple structures with respect to the normal theoretical vector was 0.00854, whereas with a single structure this value was 0.01117. In summary, these results show that is more stable to calculate the normal vectors using the estimation of multiple structures versus single structure, even with the presence of noise.</p></sec><sec id="sec3dot4-sensors-17-02262"><title>3.4. Evaluation of the Discriminating Properties</title><p>In order to evaluate the discriminating capabilities of the MPFH method, it is compared with the PFH and FPFH descriptor with different <italic>k</italic> neighborhood support. The measure of similarity used for the histogram was the intersection between the descriptors for seven different regions: the point in a plane, cylinder, sphere, a point on the vertices of a triangular pyramid, square pyramid, pentagonal pyramid, and hexagonal pyramid. When the number of sides of the polygon of the base of the pyramid increases, the curvature of the vertex is close to the curvature of a sphere. These regions were chosen because they present a small change between them in the curvature.</p><p><xref ref-type="fig" rid="sensors-17-02262-f012">Figure 12</xref> presents a similarity matrix. The intensity level represents the quantification of similarity between objects row and column. The rows correspond to the similarity measure for the PFH, FPFH, and MPFH descriptors consecutively, and the columns represent the similarity with different <inline-formula><mml:math id="mm145"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm146"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> neighborhood support. An ideal similarity matrix for a classification process would have a diagonal with black and the rest white.</p><p>It can be seen in the gray levels from <xref ref-type="fig" rid="sensors-17-02262-f012">Figure 12</xref>g&#x02013;i that there is a lower probability of confusion of the MPFH method between different classes with respect to the PFH (<xref ref-type="fig" rid="sensors-17-02262-f012">Figure 12</xref>a&#x02013;c) and FPFH (<xref ref-type="fig" rid="sensors-17-02262-f012">Figure 12</xref>d&#x02013;f). These results suggest that the description made by the MPFH method facilitates a correct classification of the underlying region.</p></sec><sec id="sec3dot5-sensors-17-02262"><title>3.5. Comparison with Some 3D Local Descriptors</title><p>In this section, the descriptor MPFH proposed is compared with some of the descriptors more often used in the literature like PFH, FPFH, spin image, radius-based surface (RSD), and signature of histograms of orientations (SHOT). From the database we carefully selected 2800 points belonging to different 3D surfaces regions. These points were categorized into five classes: hollow, crest, edge hollow, base crest and planar surfaces. Descriptors were evaluated in a classification task varying Gaussian noise, adding 0%, 5% and 10%.</p><p><xref ref-type="fig" rid="sensors-17-02262-f013">Figure 13</xref>a compares the accuracy obtained between the descriptors. It can be seen that the accuracy of the descriptor MPFH is higher, even with the addition of noise, and although accuracy decreases with increasing the percentage of noise, it provides less of a slope than the PFH, FPFH, spin image and the radius-based surface descriptor (RSD).</p><p>In <xref ref-type="fig" rid="sensors-17-02262-f013">Figure 13</xref>b, the classification results varying the <inline-formula><mml:math id="mm147"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> support radius for the calculation of the descriptors are displayed. We can also observe that while decreasing the classification accuracy with the variation in radius <inline-formula><mml:math id="mm148"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> support, our method still has the greatest accuracy between descriptors. In <xref ref-type="fig" rid="sensors-17-02262-f013">Figure 13</xref>b, the qualitative results taking the 3D sampled surface of the artificial tooth, welding defect, and simulated defects models with added noise as test data are shown.</p></sec><sec id="sec3dot6-sensors-17-02262"><title>3.6. Performance Evaluation for Defects Recognition</title><p>This section evaluates the full methodology of surface defect recognition proposed in this paper. The set of training and test images consisted of 2160 regions labeled as holes, bumps, cracks and without defect.</p><p>In the detection stage, the points of the surface are classified into five primitives, the points belonging to a hollow (red), hollow edge (yellow), crest (magenta), base crest (blue), and flat (green) surfaces. The capacity of the local descriptor to represent and classify the regions of the surface can be seen in <xref ref-type="fig" rid="sensors-17-02262-f014">Figure 14</xref>, . However, it is also observed that misclassified primitives appear, which will affect the classification process.</p><p>The recognition stage was evaluated with the classification techniques: k-nearest neighbors (knn), linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), multilayer perceptron, support vector machine (SVM) with radial basis function kernel (rbf), SVM with polynomial kernel function (poly), and SVM with a quadratic kernel (quad). In the evaluation process, we used 10-fold cross-validation. <xref ref-type="table" rid="sensors-17-02262-t003">Table 3</xref> shows the performance of each classifier. The performance was measured using the accuracy factor (Equation (<xref ref-type="disp-formula" rid="FD20-sensors-17-02262">20</xref>)).
<disp-formula id="FD20-sensors-17-02262"><label>(20)</label><mml:math id="mm149"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>l</mml:mi></mml:msubsup><mml:mfrac><mml:mrow><mml:mi>t</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mi>l</mml:mi></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The parameters (<inline-formula><mml:math id="mm150"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) are the true positive for class <inline-formula><mml:math id="mm151"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the elements (<inline-formula><mml:math id="mm152"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) are false positives, (<inline-formula><mml:math id="mm153"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) are false negatives, and finally, the the parameters (<inline-formula><mml:math id="mm154"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) are the true negatives. The factor <italic>l</italic> is the class number.</p><p><xref ref-type="fig" rid="sensors-17-02262-f015">Figure 15</xref> shows the result of the classification for different images in the database. It can be seen from images that the recognition stage is able to correctly classify the defect. However, can be seen that in defective regions close to the edges of the object, our method erroneously classifies the region as without defect (<xref ref-type="fig" rid="sensors-17-02262-f015">Figure 15</xref>d). The misclassification could be attributed to the very low density of points in this regions, which causes a misclassification in primitives, affecting the recognition.</p><p>The average processing time of the automated inspection system is composed of the acquisition step, which is delayed 0.3 s, the processing and description step with 1.1 s for a region with 1817 points and three defects, and the classification step with 0.2 s. According to the above, the automated inspection system could detect defects on a surface in 1.6 s.</p></sec></sec><sec id="sec4-sensors-17-02262"><title>4. Conclusions</title><p>In this paper, we have presented a new local 3D surface descriptor that improves the robustness and discriminating capabilities of the PFH method. We have demonstrated its applicability for surface quality inspection in the detection of defects on different objects. The proposed MPFH method was evaluated and compared to some of the most used state-of-the-art descriptors under a classification task. The obtained results show that our method has higher discriminating capabilities, because we tried to capture the geometric information of the underlying 3D surface more precisely by estimating the normals from the models that are adjusted to the surface and including these in the construction of the descriptor. From the identification of primitives, we propose a method of description that projects each connected component of primitives on a plane forming the 2D image. Then, 2D geometric features are extracted to recognize the defect. With this method, three types of defects (holes, bumps, and cracks), with an accuracy of 94.17%, are recognized using an SVM. In the MPFH descriptor calculation, the models that best fit the surface are estimated through an estimation technique of multiple structures or models. In this paper the models are flat surfaces, however, we believe that if models can be fitted to polynomial surfaces or splines, this would help to estimate the normal vectors more correctly and the local region would be more accurately represented.</p></sec></body><back><ack><title>Acknowledgments</title><p>We give particular thanks to the research groups Automatic, Electronic, and Computer Sciences at ITM, GIDIA at the Universidad Nacional de Colombia at Medellin, and GRIMA at the Pontificia Universidad Cat&#x000f3;lica de Chile.</p></ack><notes><title>Author Contributions</title><p>Carlos A. Madrigal contributed extensively to the entire work, in particular to the concept and implementation of the MPFH descriptor and the writing of the paper. John W. Branch and Alejandro Restrepo contributed to the work as scientific directors, designing the methodology of recognition, analyzing the results, and preparing the paper. The contribution of Domingo Mery focused on the concept of the descriptor MPFH, the analysis of results and feedback on the manuscript.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-17-02262"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaramillo</surname><given-names>A.</given-names></name><name><surname>Prieto</surname><given-names>F.</given-names></name><name><surname>Boulanger</surname><given-names>P.</given-names></name></person-group><article-title>Fast dimensional inspection of deformable parts from partial views</article-title><source>Comput. Ind.</source><year>2013</year><volume>64</volume><fpage>1076</fpage><lpage>1081</lpage><pub-id pub-id-type="doi">10.1016/j.compind.2013.03.016</pub-id></element-citation></ref><ref id="B2-sensors-17-02262"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aubreton</surname><given-names>O.</given-names></name><name><surname>Bajard</surname><given-names>A.</given-names></name><name><surname>Verney</surname><given-names>B.</given-names></name><name><surname>Truchetet</surname><given-names>F.</given-names></name></person-group><article-title>Infrared system for 3D scanning of metallic surfaces</article-title><source>Mach. Vis. Appl.</source><year>2013</year><volume>24</volume><fpage>1513</fpage><lpage>1524</lpage><pub-id pub-id-type="doi">10.1007/s00138-013-0487-z</pub-id></element-citation></ref><ref id="B3-sensors-17-02262"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bokhabrine</surname><given-names>Y.</given-names></name><name><surname>Seulin</surname><given-names>R.</given-names></name><name><surname>Voon</surname><given-names>L.F.L.Y.</given-names></name><name><surname>Gorria</surname><given-names>P.</given-names></name><name><surname>Girardin</surname><given-names>G.</given-names></name><name><surname>Gomez</surname><given-names>M.</given-names></name><name><surname>Jobard</surname><given-names>D.</given-names></name></person-group><article-title>3D characterization of hot metallic shells during industrial forging</article-title><source>Mach. Vis. Appl.</source><year>2012</year><volume>23</volume><fpage>417</fpage><lpage>425</lpage><pub-id pub-id-type="doi">10.1007/s00138-010-0297-5</pub-id></element-citation></ref><ref id="B4-sensors-17-02262"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malamas</surname><given-names>E.N.</given-names></name><name><surname>Petrakis</surname><given-names>E.G.</given-names></name><name><surname>Zervakis</surname><given-names>M.</given-names></name><name><surname>Petit</surname><given-names>L.</given-names></name><name><surname>Legat</surname><given-names>J.D.</given-names></name></person-group><article-title>A survey on industrial vision systems, applications and tools</article-title><source>Image Vis. Comput.</source><year>2003</year><volume>21</volume><fpage>171</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1016/S0262-8856(02)00152-X</pub-id></element-citation></ref><ref id="B5-sensors-17-02262"><label>5.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Leach</surname><given-names>R.K.</given-names></name></person-group><source>Optical Measurement of Surface Topography</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin, Germany</publisher-loc><year>2011</year><fpage>71</fpage><lpage>106</lpage></element-citation></ref><ref id="B6-sensors-17-02262"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neogi</surname><given-names>N.</given-names></name><name><surname>Mohanta</surname><given-names>D.K.</given-names></name><name><surname>Dutta</surname><given-names>P.K.</given-names></name></person-group><article-title>Review of vision-based steel surface inspection systems</article-title><source>EURASIP J. Image Video Process.</source><year>2014</year><volume>50</volume><pub-id pub-id-type="doi">10.1186/1687-5281-2014-50</pub-id></element-citation></ref><ref id="B7-sensors-17-02262"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>H.</given-names></name><name><surname>Li</surname><given-names>S.</given-names></name><name><surname>Gu</surname><given-names>D.</given-names></name><name><surname>Chang</surname><given-names>H.</given-names></name></person-group><article-title>Bearing defect inspection based on machine vision</article-title><source>Measurement</source><year>2012</year><volume>45</volume><fpage>719</fpage><lpage>733</lpage><pub-id pub-id-type="doi">10.1016/j.measurement.2011.12.018</pub-id></element-citation></ref><ref id="B8-sensors-17-02262"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhi</surname><given-names>H.</given-names></name><name><surname>Johansson</surname><given-names>R.B.</given-names></name></person-group><article-title>Interpretation and classification of fringe patterns</article-title><source>Opt. Lasers Eng.</source><year>1992</year><volume>17</volume><fpage>9</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1016/0143-8166(92)90023-Z</pub-id></element-citation></ref><ref id="B9-sensors-17-02262"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caulier</surname><given-names>Y.</given-names></name><name><surname>Spinnler</surname><given-names>K.</given-names></name><name><surname>Wittenberg</surname><given-names>T.</given-names></name><name><surname>Bourennane</surname><given-names>S.</given-names></name></person-group><article-title>Specific features for the analysis of fringe images</article-title><source>Opt. Eng.</source><year>2008</year><volume>47</volume><fpage>057201</fpage><pub-id pub-id-type="doi">10.1117/1.2927463</pub-id></element-citation></ref><ref id="B10-sensors-17-02262"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Osten</surname><given-names>W.</given-names></name><name><surname>Jueptner</surname><given-names>W.P.</given-names></name><name><surname>Mieth</surname><given-names>U.</given-names></name></person-group><article-title>Knowledge-assisted evaluation of fringe patterns for automatic fault detection</article-title><source>Proceedings of the SPIE&#x02019;s 1993 International Symposium on Optics, Imaging, and Instrumentation</source><conf-loc>San Diego, CA, USA</conf-loc><conf-date>11&#x02013;16 July 1994</conf-date><fpage>256</fpage><lpage>268</lpage></element-citation></ref><ref id="B11-sensors-17-02262"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Ai</surname><given-names>J.</given-names></name><name><surname>Sun</surname><given-names>C.</given-names></name></person-group><article-title>Online fabric defect inspection using smart visual sensors</article-title><source>Sensors</source><year>2013</year><volume>13</volume><fpage>4659</fpage><lpage>4673</lpage><pub-id pub-id-type="doi">10.3390/s130404659</pub-id><?supplied-pmid 23571669?><pub-id pub-id-type="pmid">23571669</pub-id></element-citation></ref><ref id="B12-sensors-17-02262"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mart&#x000ed;nez</surname><given-names>S.S.</given-names></name><name><surname>Ortega</surname><given-names>J.G.</given-names></name><name><surname>Garc&#x000ed;a</surname><given-names>J.G.</given-names></name><name><surname>Garc&#x000ed;a</surname><given-names>A.S.</given-names></name><name><surname>Est&#x000e9;vez</surname><given-names>E.E.</given-names></name></person-group><article-title>An industrial vision system for surface quality inspection of transparent parts</article-title><source>Int. J. Adv. Manuf. Technol.</source><year>2013</year><volume>68</volume><fpage>1123</fpage><lpage>1136</lpage><pub-id pub-id-type="doi">10.1007/s00170-013-4904-2</pub-id></element-citation></ref><ref id="B13-sensors-17-02262"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mart&#x000ed;nez</surname><given-names>S.S.</given-names></name><name><surname>V&#x000e1;zquez</surname><given-names>C.O.</given-names></name><name><surname>Garc&#x000ed;a</surname><given-names>J.G.</given-names></name><name><surname>Ortega</surname><given-names>J.G.</given-names></name></person-group><article-title>Quality inspection of machined metal parts using an image fusion technique</article-title><source>Measurement</source><year>2017</year><volume>111</volume><fpage>374</fpage><lpage>383</lpage><pub-id pub-id-type="doi">10.1016/j.measurement.2017.08.002</pub-id></element-citation></ref><ref id="B14-sensors-17-02262"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pernkopf</surname><given-names>F.</given-names></name></person-group><article-title>3D surface acquisition and reconstruction for inspection of raw steel products</article-title><source>Comput. Ind.</source><year>2005</year><volume>56</volume><fpage>876</fpage><lpage>885</lpage><pub-id pub-id-type="doi">10.1016/j.compind.2005.05.025</pub-id></element-citation></ref><ref id="B15-sensors-17-02262"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ogun</surname><given-names>P.S.</given-names></name><name><surname>Chamberlain</surname><given-names>M.R.</given-names></name><name><surname>Phairatt</surname><given-names>P.L.</given-names></name><name><surname>Tailor</surname><given-names>M.J.</given-names></name><name><surname>Jackson</surname><given-names>M.R.</given-names></name></person-group><article-title>An active three-dimensional vision system for automated detection and measurement of surface defects</article-title><source>Proc. Inst. Mech. Eng. Part B J. Eng. Manuf.</source><year>2014</year><volume>228</volume><fpage>1543</fpage><lpage>1549</lpage><pub-id pub-id-type="doi">10.1177/0954405414522604</pub-id></element-citation></ref><ref id="B16-sensors-17-02262"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chu</surname><given-names>H.H.</given-names></name><name><surname>Wang</surname><given-names>Z.Y.</given-names></name></person-group><article-title>A vision-based system for post-welding quality measurement and defect detection</article-title><source>Int. J. Adv. Manuf. Technol.</source><year>2016</year><volume>86</volume><fpage>3007</fpage><lpage>3014</lpage><pub-id pub-id-type="doi">10.1007/s00170-015-8334-1</pub-id></element-citation></ref><ref id="B17-sensors-17-02262"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>L.M.</given-names></name><name><surname>Li</surname><given-names>Z.Y.</given-names></name><name><surname>Chang</surname><given-names>Y.L.</given-names></name><name><surname>Xing</surname><given-names>G.X.</given-names></name><name><surname>Wang</surname><given-names>P.Q.</given-names></name><name><surname>Xi</surname><given-names>J.T.</given-names></name><name><surname>Zhu</surname><given-names>T.D.</given-names></name></person-group><article-title>A color phase shift profilometry for the fabric defect detection</article-title><source>Optoelectron. Lett.</source><year>2014</year><volume>10</volume><fpage>308</fpage><lpage>312</lpage><pub-id pub-id-type="doi">10.1007/s11801-014-4065-z</pub-id></element-citation></ref><ref id="B18-sensors-17-02262"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodriguez-Martin</surname><given-names>M.</given-names></name><name><surname>Rodriguez-Gonzalvez</surname><given-names>P.</given-names></name><name><surname>Gonzalez-Aguilera</surname><given-names>D.</given-names></name><name><surname>Fernandez-Hernandez</surname><given-names>J.</given-names></name></person-group><article-title>Feasibility study of a structured light system applied to welding inspection based on articulated coordinate measure machine data</article-title><source>IEEE Sens. J.</source><year>2017</year><volume>17</volume><fpage>4217</fpage><lpage>4224</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2017.2700954</pub-id></element-citation></ref><ref id="B19-sensors-17-02262"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chua</surname><given-names>C.S.</given-names></name><name><surname>Jarvis</surname><given-names>R.</given-names></name></person-group><article-title>Point signatures: A new representation for 3D object recognition</article-title><source>Int. J. Comput. Vis.</source><year>1997</year><volume>25</volume><fpage>63</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1023/A:1007981719186</pub-id></element-citation></ref><ref id="B20-sensors-17-02262"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>A.E.</given-names></name><name><surname>Hebert</surname><given-names>M.</given-names></name></person-group><article-title>Using spin images for efficient object recognition in cluttered 3D scenes</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>1999</year><volume>21</volume><fpage>433</fpage><lpage>449</lpage><pub-id pub-id-type="doi">10.1109/34.765655</pub-id></element-citation></ref><ref id="B21-sensors-17-02262"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rusu</surname><given-names>R.B.</given-names></name><name><surname>Blodow</surname><given-names>N.</given-names></name><name><surname>Marton</surname><given-names>Z.C.</given-names></name><name><surname>Beetz</surname><given-names>M.</given-names></name></person-group><article-title>Aligning point cloud views using persistent feature histograms</article-title><source>Proceedings of the 2008 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source><conf-loc>Nice, France</conf-loc><conf-date>22&#x02013;26 September 2008</conf-date><fpage>3384</fpage><lpage>3391</lpage></element-citation></ref><ref id="B22-sensors-17-02262"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rusu</surname><given-names>R.B.</given-names></name><name><surname>Blodow</surname><given-names>N.</given-names></name><name><surname>Beetz</surname><given-names>M.</given-names></name></person-group><article-title>Fast point feature histograms (FPFH) for 3D registration</article-title><source>Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)</source><conf-loc>Kobe, Japan</conf-loc><conf-date>12&#x02013;17 May 2009</conf-date><fpage>3212</fpage><lpage>3217</lpage></element-citation></ref><ref id="B23-sensors-17-02262"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rusu</surname><given-names>R.B.</given-names></name><name><surname>Bradski</surname><given-names>G.</given-names></name><name><surname>Thibaux</surname><given-names>R.</given-names></name><name><surname>Hsu</surname><given-names>J.</given-names></name></person-group><article-title>Fast 3D recognition and pose using the viewpoint feature histogram</article-title><source>Proceedings of the 2010 IEEE/RSJ International Conference on the Intelligent Robots and Systems (IROS)</source><conf-loc>Taipei, Taiwan</conf-loc><conf-date>18&#x02013;22 October 2010</conf-date><fpage>2155</fpage><lpage>2162</lpage></element-citation></ref><ref id="B24-sensors-17-02262"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Toldo</surname><given-names>R.</given-names></name><name><surname>Fusiello</surname><given-names>A.</given-names></name></person-group><article-title>Robust multiple structures estimation with J-linkage</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Marseille, France</conf-loc><conf-date>12&#x02013;18 October 2008</conf-date><fpage>537</fpage><lpage>547</lpage></element-citation></ref><ref id="B25-sensors-17-02262"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Madrigal</surname><given-names>C.A.</given-names></name><name><surname>Restrepo</surname><given-names>A.</given-names></name><name><surname>Branch</surname><given-names>J.W.</given-names></name></person-group><article-title>Identification of superficial defects in reconstructed 3D objects using phase-shifting fringe projection</article-title><source>Proceedings of the SPIE Optical Engineering + Applications</source><conf-loc>San Diego, CA, USA</conf-loc><conf-date>28 September&#x02013;1 October 2016</conf-date></element-citation></ref><ref id="B26-sensors-17-02262"><label>26.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hoppe</surname><given-names>H.</given-names></name><name><surname>DeRose</surname><given-names>T.</given-names></name><name><surname>Duchamp</surname><given-names>T.</given-names></name><name><surname>McDonald</surname><given-names>J.</given-names></name><name><surname>Stuetzle</surname><given-names>W.</given-names></name></person-group><source>Surface Reconstruction from Unorganized Points</source><publisher-name>ACM</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>1992</year></element-citation></ref><ref id="B27-sensors-17-02262"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischler</surname><given-names>M.A.</given-names></name><name><surname>Bolles</surname><given-names>R.C.</given-names></name></person-group><article-title>Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography</article-title><source>Commun. ACM</source><year>1981</year><volume>24</volume><fpage>381</fpage><lpage>395</lpage><pub-id pub-id-type="doi">10.1145/358669.358692</pub-id></element-citation></ref><ref id="B28-sensors-17-02262"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Fouhey</surname><given-names>D.F.</given-names></name><name><surname>Scharstein</surname><given-names>D.</given-names></name><name><surname>Briggs</surname><given-names>A.J.</given-names></name></person-group><article-title>Multiple plane detection in image pairs using J-linkage</article-title><source>Proceedings of the 20th International Conference on Pattern Recognition (ICPR)</source><conf-loc>Istanbul, Turkey</conf-loc><conf-date>23&#x02013;26 August 2010</conf-date><fpage>336</fpage><lpage>339</lpage></element-citation></ref><ref id="B29-sensors-17-02262"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tardif</surname><given-names>J.P.</given-names></name></person-group><article-title>Non-iterative approach for fast and accurate vanishing point detection</article-title><source>Proceedings of the IEEE 12th International Conference on Computer Vision</source><conf-loc>Kyoto, Japan</conf-loc><conf-date>29 September&#x02013;2 October 2009</conf-date><fpage>1250</fpage><lpage>1257</lpage></element-citation></ref><ref id="B30-sensors-17-02262"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berkmann</surname><given-names>J.</given-names></name><name><surname>Caelli</surname><given-names>T.</given-names></name></person-group><article-title>Computation of surface geometry and segmentation using covariance techniques</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>1994</year><volume>16</volume><fpage>1114</fpage><lpage>1116</lpage><pub-id pub-id-type="doi">10.1109/34.334391</pub-id></element-citation></ref><ref id="B31-sensors-17-02262"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wahl</surname><given-names>E.</given-names></name><name><surname>Hillenbrand</surname><given-names>U.</given-names></name><name><surname>Hirzinger</surname><given-names>G.</given-names></name></person-group><article-title>Surflet-pair-relation histograms: A statistical 3D-shape representation for rapid classification</article-title><source>Proceedings of the Fourth International Conference on 3-D Digital Imaging and Modeling (3DIM)</source><conf-loc>Banff, AB, Canada</conf-loc><conf-date>6&#x02013;10 October 2003</conf-date><fpage>474</fpage><lpage>481</lpage></element-citation></ref><ref id="B32-sensors-17-02262"><label>32.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>Y.</given-names></name><name><surname>Kramer</surname><given-names>T.</given-names></name><name><surname>Brown</surname><given-names>R.</given-names></name><name><surname>Xu</surname><given-names>X.</given-names></name></person-group><source>Information Modeling for Interoperable Dimensional Metrology</source><publisher-name>Springer Science &#x00026; Business Media</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2011</year></element-citation></ref><ref id="B33-sensors-17-02262"><label>33.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Mika</surname><given-names>S.</given-names></name><name><surname>Ratsch</surname><given-names>G.</given-names></name><name><surname>Weston</surname><given-names>J.</given-names></name><name><surname>Scholkopf</surname><given-names>B.</given-names></name><name><surname>Mullers</surname><given-names>K.R.</given-names></name></person-group><article-title>Fisher discriminant analysis with kernels</article-title><source>Proceedings of the 1999 IEEE Signal Processing Society Workshop Neural Networks for Signal Processing</source><conf-loc>Madison, WI, USA</conf-loc><conf-date>25 August 1999</conf-date><fpage>41</fpage><lpage>48</lpage></element-citation></ref><ref id="B34-sensors-17-02262"><label>34.</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Mery</surname><given-names>D.</given-names></name></person-group><article-title>BALU: A Toolbox Matlab for Computer Vision, Pattern Recognition and Image Processing</article-title><year>2011</year><comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://bibbase.org/network/publication/mery-baluamatlabtoolboxforcomputervisionpatternrecognitionandimageprocessing-2011">http://bibbase.org/network/publication/mery-baluamatlabtoolboxforcomputervisionpatternrecognitionandimageprocessing-2011</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2016-11-15">(accessed on 15 November 2016)</date-in-citation></element-citation></ref></ref-list></back><floats-group><fig id="sensors-17-02262-f001" orientation="portrait" position="float"><label>Figure 1</label><caption><p>The overview of our system. (<bold>a</bold>) Calibration and 3D reconstruction using structured light; (<bold>b</bold>) representation through the point cloud; (<bold>c</bold>) calculation of multiple structures; (<bold>d</bold>) estimation of normal vectors by principal component analysis; (<bold>e</bold>) calculation of the contribution of each model to the formation of the region; (<bold>f</bold>) construction of a histogram as a 3D local descriptor; (<bold>g</bold>) classification of the point cloud in primitives; (<bold>h</bold>) surface defect recognition. MPFH: Model Points Feature Histogram.</p></caption><graphic xlink:href="sensors-17-02262-g001"/></fig><fig id="sensors-17-02262-f002" orientation="portrait" position="float"><label>Figure 2</label><caption><p>Scheme of the 3D reconstruction system.</p></caption><graphic xlink:href="sensors-17-02262-g002"/></fig><fig id="sensors-17-02262-f003" orientation="portrait" position="float"><label>Figure 3</label><caption><p>(<bold>a</bold>) Comparison of calculation of normals in edge and corner using the estimation of one and multiple structures; (<bold>b</bold>) calculation of normals for a cube using J-linkage for estimating multiple structures; (<bold>c</bold>) estimated model centers on a defective surface.</p></caption><graphic xlink:href="sensors-17-02262-g003"/></fig><fig id="sensors-17-02262-f004" orientation="portrait" position="float"><label>Figure 4</label><caption><p>Normal estimation on hole defect by the proposed methodology.</p></caption><graphic xlink:href="sensors-17-02262-g004"/></fig><fig id="sensors-17-02262-f005" orientation="portrait" position="float"><label>Figure 5</label><caption><p>Darboux features. (<bold>a</bold>) Darboux coordinate system. (<bold>b</bold>) Procedure for the construction of the MPFH descriptor.</p></caption><graphic xlink:href="sensors-17-02262-g005"/></fig><fig id="sensors-17-02262-f006" orientation="portrait" position="float"><label>Figure 6</label><caption><p>Procedure for the construction of the MPFH descriptor of a point belonging to a flat region and of a point belonging to a hole-like defect.</p></caption><graphic xlink:href="sensors-17-02262-g006"/></fig><fig id="sensors-17-02262-f007" orientation="portrait" position="float"><label>Figure 7</label><caption><p>Scheme recognition. SVM: Support vector machine.</p></caption><graphic xlink:href="sensors-17-02262-g007"/></fig><fig id="sensors-17-02262-f008" orientation="portrait" position="float"><label>Figure 8</label><caption><p>A 2D image of the defect. (<bold>a</bold>) Estimation of projection plane; (<bold>b</bold>) primitive projection; (<bold>c</bold>) a 2D image of primitives; (<bold>d</bold>) a 2D projection of a defect on an artificial tooth; (<bold>e</bold>) a 2D projection of a defect on a simulated surface.</p></caption><graphic xlink:href="sensors-17-02262-g008"/></fig><fig id="sensors-17-02262-f009" orientation="portrait" position="float"><label>Figure 9</label><caption><p>Examples from the database in: (<bold>a</bold>) defect in welding; (<bold>b</bold>) indentations in materials; (<bold>c</bold>) artificial teeth; (<bold>d</bold>) models of artificial cracks; (<bold>e</bold>) ceramics; (<bold>f</bold>) simulations.</p></caption><graphic xlink:href="sensors-17-02262-g009"/></fig><fig id="sensors-17-02262-f010" orientation="portrait" position="float"><label>Figure 10</label><caption><p>Surface defects. (<bold>a</bold>) A 2D image of an indentation with a test of Vickers hardness; (<bold>b</bold>) a scanning electron microscope (SEM) image of an indentation; (<bold>c</bold>) a 3D reconstruction of an indentation; (<bold>d</bold>) a 2D image of a detachment defect in an artificial tooth; (<bold>e</bold>) an SEM image of an artificial tooth; (<bold>f</bold>) a 3D reconstruction of an artificial tooth.</p></caption><graphic xlink:href="sensors-17-02262-g010"/></fig><fig id="sensors-17-02262-f011" orientation="portrait" position="float"><label>Figure 11</label><caption><p>Comparison of adjustment of normal using one or multiple structures varying the number of <italic>k</italic>-neighbors.</p></caption><graphic xlink:href="sensors-17-02262-g011"/></fig><fig id="sensors-17-02262-f012" orientation="portrait" position="float"><label>Figure 12</label><caption><p>Measurement of distances between feature vectors of points in different regions and with different <inline-formula><mml:math id="mm155"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm156"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> neighborhood support. (<bold>a</bold>) PFH con <inline-formula><mml:math id="mm157"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm158"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>45</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>; (<bold>b</bold>) PFH <inline-formula><mml:math id="mm159"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm160"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>; (<bold>c</bold>) PFH <inline-formula><mml:math id="mm161"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>45</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm162"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>75</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>; (<bold>d</bold>) FPFH <inline-formula><mml:math id="mm163"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm164"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>45</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>; (<bold>e</bold>) FPFH <inline-formula><mml:math id="mm165"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm166"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>; (<bold>f</bold>) FPFH <inline-formula><mml:math id="mm167"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>45</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm168"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>75</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>; (<bold>g</bold>) MPFH <inline-formula><mml:math id="mm169"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm170"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>45</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>; (<bold>g</bold>) MPFH <inline-formula><mml:math id="mm171"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm172"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and (<bold>i</bold>) MPFH <inline-formula><mml:math id="mm173"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>45</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm174"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>75</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></caption><graphic xlink:href="sensors-17-02262-g012a"/><graphic xlink:href="sensors-17-02262-g012b"/></fig><fig id="sensors-17-02262-f013" orientation="portrait" position="float"><label>Figure 13</label><caption><p>MPFH Descriptor. (<bold>a</bold>) Comparing different descriptors adding Gaussian noise; (<bold>b</bold>) comparison of different descriptors varying the <inline-formula><mml:math id="mm175"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> support radius.</p></caption><graphic xlink:href="sensors-17-02262-g013"/></fig><fig id="sensors-17-02262-f014" orientation="portrait" position="float"><label>Figure 14</label><caption><p>Classification of primitives on different objects. (<bold>a</bold>) artificial teeth; (<bold>b</bold>) ceramics; (<bold>c</bold>) models of artificial defects; (<bold>d</bold>) defects in welding.</p></caption><graphic xlink:href="sensors-17-02262-g014"/></fig><fig id="sensors-17-02262-f015" orientation="portrait" position="float"><label>Figure 15</label><caption><p>Qualitative results for the recognition of defects in (<bold>a</bold>) Indentation with a test of Vickers hardness; (<bold>b</bold>) Models of artificial defects; (<bold>c</bold>) Welding; (<bold>d</bold>) Artificial teeth. The orange region represents crack-type defects, the green region represents bumps-type defects, the purple region represents hole-type defects, and the gray color represents the regions without defects.</p></caption><graphic xlink:href="sensors-17-02262-g015"/></fig><table-wrap id="sensors-17-02262-t001" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-17-02262-t001_Table 1</object-id><label>Table 1</label><caption><p>Camera and digital light projector specifications.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Device</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Specifications</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Camera</td><td align="center" valign="middle" rowspan="1" colspan="1">Point Grey FL3-U3-88S2C-C</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Sensor size</td><td align="center" valign="middle" rowspan="1" colspan="1">1.55 <inline-formula><mml:math id="mm176"><mml:mrow><mml:mi mathvariant="sans-serif">&#x003bc;</mml:mi></mml:mrow></mml:math></inline-formula>m &#x000d7; 1.55 <inline-formula><mml:math id="mm177"><mml:mrow><mml:mi mathvariant="sans-serif">&#x003bc;</mml:mi></mml:mrow></mml:math></inline-formula>m</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Image resolution</td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm178"><mml:mrow><mml:mrow><mml:mn>4096</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>2160</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> pixels</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Lenses</td><td align="center" valign="middle" rowspan="1" colspan="1">Edmund Optics <italic>f</italic>8.5 mm</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Digital light projector (DLP)</td><td align="center" valign="middle" rowspan="1" colspan="1">DLP LightCrafter 4500</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DLP resolution</td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm179"><mml:mrow><mml:mrow><mml:mn>912</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1140</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> pixels</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Synchronization circuit</td><td align="center" valign="middle" rowspan="1" colspan="1">freescale FRDM-K20D50M</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Software</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Visual C++ + OpenCV3.0 + PCL1.8</td></tr></tbody></table></table-wrap><table-wrap id="sensors-17-02262-t002" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-17-02262-t002_Table 2</object-id><label>Table 2</label><caption><p>Normal vectors in corners, edges, and planes of a cube, using single (standard methods) and multiple structures (method used).</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" colspan="1">Region</th><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">Theoretical Normal Vector</th><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">One-Structure Theoretical</th><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">Multiple-Structure J-linkage</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm180"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold-italic">N</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math></inline-formula>/<inline-formula><mml:math id="mm181"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold-italic">N</mml:mi><mml:mi mathvariant="bold-italic">y</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math></inline-formula>/<inline-formula><mml:math id="mm182"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold-italic">N</mml:mi><mml:mi mathvariant="bold-italic">z</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(Standard Methods) <inline-formula><mml:math id="mm183"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold-italic">N</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math></inline-formula>/<inline-formula><mml:math id="mm184"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold-italic">N</mml:mi><mml:mi mathvariant="bold-italic">y</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math></inline-formula>/<inline-formula><mml:math id="mm185"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold-italic">N</mml:mi><mml:mi mathvariant="bold-italic">z</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Theoretical (Method Used) <inline-formula><mml:math id="mm186"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold-italic">N</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math></inline-formula>/<inline-formula><mml:math id="mm187"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold-italic">N</mml:mi><mml:mi mathvariant="bold-italic">y</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math></inline-formula>/<inline-formula><mml:math id="mm188"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold-italic">N</mml:mi><mml:mi mathvariant="bold-italic">z</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math></inline-formula></th></tr></thead><tbody><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Corner</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0/&#x02212;1.0/0.0</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02212;0.5350/&#x02212;0.5350/0.6538</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0/&#x02212;1.0/0.0</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#x02212;1.0/0.0/0.0</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02014;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02212;1.0/0.0/0.0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0/0.0/1.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02014;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0/0.0/1.0</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Corner 5% noise</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0/&#x02212;1.0/0.0</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02212;0.5438/&#x02212;0.5888/0.5979</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02212;0.0010/&#x02212;0.9997/0.0215</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#x02212;1.0/0.0/0.0</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02014;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02212;0.9993/0.0234/0.02866</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0/0.0/1.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02014;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0711/0.0478/0.9963</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Edge</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0/0.0/1.0</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0/&#x02212;0.7071/0.7071</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0/0.0/1.0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0/&#x02212;1.0/0.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02014;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0/&#x02212;1.0/0.0</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Edge 5% noise</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0/0.0/1.0</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0027/&#x02212;0.5286/0.8488</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0765/&#x02212;0.0416/0.9962</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0/&#x02212;1.0/0.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02014;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0159/&#x02212;0.9953/&#x02212;0.0952</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Plane</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm189"><mml:mrow><mml:mrow><mml:mn>0.0</mml:mn><mml:mo>/</mml:mo><mml:mn>0.0</mml:mn><mml:mo>/</mml:mo><mml:mn>1.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm190"><mml:mrow><mml:mrow><mml:mn>0.0</mml:mn><mml:mo>/</mml:mo><mml:mn>0.0</mml:mn><mml:mo>/</mml:mo><mml:mn>1.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm191"><mml:mrow><mml:mrow><mml:mn>0.0</mml:mn><mml:mo>/</mml:mo><mml:mn>0.0</mml:mn><mml:mo>/</mml:mo><mml:mn>1.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Plane 5% noise</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0/0.0/1.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0191/&#x02212;0.0116/0.9998</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.01271/&#x02212;0.0114/0.9998</td></tr></tbody></table></table-wrap><table-wrap id="sensors-17-02262-t003" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-17-02262-t003_Table 3</object-id><label>Table 3</label><caption><p>Results of classification of surface defects. LDA: linear discriminant analysis; knn: k-nearest neighbors; QDA: quadratic discriminant analysis; SVM (rbf): SVM with radial basis function kernel; SVM (poly): SVM with polynomial kernel function; SVM (quad): SVM with a quadratic kernel.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Classifier</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">knn (k = 10)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">LDA</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">QDA</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Multilayer Perceptron</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SVM (rbf)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SVM (poly)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SVM (quad)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.09%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">86.22%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">66.89%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.95%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.17%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.72%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.05%</td></tr></tbody></table></table-wrap></floats-group></article>