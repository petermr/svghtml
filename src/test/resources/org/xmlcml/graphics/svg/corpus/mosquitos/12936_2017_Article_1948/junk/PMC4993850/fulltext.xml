<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="other"><?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.4.0//EN//XML?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName art540.dtd?><?SourceDTD.Version 5.4.0?><?ConverterInfo.XSLTName elsevier2nlmx2.xsl?><?ConverterInfo.Version 2?><?origin publisher?><?FILEmeta_DIB939 xml ?><?FILEmain xml ?><?FILEmain pdf ?><?FILEmmc1 docx ?><?FILEmmc2 xlsx ?><front><journal-meta><journal-id journal-id-type="nlm-ta">Data Brief</journal-id><journal-id journal-id-type="iso-abbrev">Data Brief</journal-id><journal-title-group><journal-title>Data in Brief</journal-title></journal-title-group><issn pub-type="epub">2352-3409</issn><publisher><publisher-name>Elsevier</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">4993850</article-id><article-id pub-id-type="publisher-id">S2352-3409(16)30430-9</article-id><article-id pub-id-type="doi">10.1016/j.dib.2016.06.062</article-id><article-categories><subj-group subj-group-type="heading"><subject>Data Article</subject></subj-group></article-categories><title-group><article-title>Dataset of embodied perspective enhances self and friend-biases in perceptual matching</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Sun</surname><given-names>Yang</given-names></name><email>luciferjingqi@gmail.com</email><xref rid="aff0005" ref-type="aff">a</xref><xref rid="cor1" ref-type="corresp">&#x0204e;</xref></contrib><contrib contrib-type="author"><name><surname>Fuentes</surname><given-names>Luis J.</given-names></name><xref rid="aff0010" ref-type="aff">b</xref><xref rid="aff0015" ref-type="aff">c</xref></contrib><contrib contrib-type="author"><name><surname>Humphreys</surname><given-names>Glyn W.</given-names></name><xref rid="aff0020" ref-type="aff">d</xref></contrib><contrib contrib-type="author"><name><surname>Sui</surname><given-names>Jie</given-names></name><xref rid="aff0005" ref-type="aff">a</xref><xref rid="aff0020" ref-type="aff">d</xref></contrib></contrib-group><aff id="aff0005"><label>a</label>Department of Psychology, Tsinghua University, Beijing 100084, PR China</aff><aff id="aff0010"><label>b</label>Department of Basic Psychology and Methodology, University of Murcia, Spain</aff><aff id="aff0015"><label>c</label>Universidad Aut&#x000f3;noma de Chile, Chile</aff><aff id="aff0020"><label>d</label>Department of Experimental Psychology, University of Oxford, South Parks Road, Oxford OX1 3UD, UK</aff><author-notes><corresp id="cor1"><label>&#x0204e;</label>Corresponding author at: Department of Psychology, Tsinghua University, Beijing 100084, PR China. <email>luciferjingqi@gmail.com</email></corresp></author-notes><pub-date pub-type="pmc-release"><day>05</day><month>7</month><year>2016</year></pub-date><!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.--><pub-date pub-type="collection"><month>9</month><year>2016</year></pub-date><pub-date pub-type="epub"><day>05</day><month>7</month><year>2016</year></pub-date><volume>8</volume><fpage>1374</fpage><lpage>1376</lpage><history><date date-type="received"><day>3</day><month>5</month><year>2016</year></date><date date-type="rev-recd"><day>15</day><month>6</month><year>2016</year></date><date date-type="accepted"><day>30</day><month>6</month><year>2016</year></date></history><permissions><copyright-statement>&#x000a9; 2016 Published by Elsevier Inc.</copyright-statement><copyright-year>2016</copyright-year><copyright-holder/><license license-type="CC BY-NC-ND" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><license-p>This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).</license-p></license></permissions><abstract id="ab0010"><p>The data article includes reaction time and accuracy from four experiments. It descries three independent variables: the social meaning of geometric shape (include self, friend and stranger), the label of identify (self, friend and stranger), the body perceptive (first-person perspective and third-person perspective), see (Sun et al., 2016)&#x000a0;<xref rid="bib1" ref-type="bibr">[1]</xref>.</p></abstract></article-meta></front><body><p><bold>Specifications Table</bold><table-wrap id="t0005" position="float"><alt-text id="at0005">Table</alt-text><table frame="hsides" rules="groups"><tbody><tr><td>Subject area</td><td><italic>Psychology</italic></td></tr><tr><td>More specific subject area</td><td><italic>Social Cognition</italic></td></tr><tr><td>Type of data</td><td><italic>Table</italic></td></tr><tr><td>How data was acquired</td><td><italic>PC with Dell Optiplex series</italic>, <italic>Intel Pentium Processor</italic> (<italic>Dual Core</italic>), <italic>Windows 7 Professional</italic>, <italic>4GB Memory</italic>, <italic>500GB Ha</italic>, <italic>rd Drive</italic>, <italic>and</italic> 22&#x02032;&#x02019; <italic>CRT displayer</italic></td></tr><tr><td>Data format</td><td><italic>Raw</italic></td></tr><tr><td>Experimental factors</td><td><italic>Three factors</italic>: <italic>personal association, matching</italic>, <italic>and perspective taking</italic></td></tr><tr><td>Experimental features</td><td><italic>To assess whether embodied perspective can enhance self and friend</italic>-<italic>biases in perceptual matching task</italic></td></tr><tr><td>Data source location</td><td><italic>Tshinghua University</italic>, <italic>Beijing</italic>, <italic>China</italic></td></tr><tr><td>Data accessibility</td><td><italic>Data is with this article</italic></td></tr></tbody></table></table-wrap></p><p><bold>Value of the data</bold><list list-type="simple"><list-item id="u0005"><label>&#x02022;</label><p>The data across four experiments can be used to estimate the statistical power in social associative learning.</p></list-item><list-item id="u0010"><label>&#x02022;</label><p>The data can be used for meta-analysis in the domain of self processing.</p></list-item><list-item id="u0015"><label>&#x02022;</label><p>The data provide a link in the research fields of self and perspective taking.</p></list-item></list></p><sec id="s0005"><label>1</label><title>Data</title><p><disp-quote><p>There are four sheets in the data file (see online version of this article <xref rid="bib1" ref-type="bibr">[1]</xref>). One sheet included the data from one experiment. In a sheet, the columns list the levels of factors and each row represents one participants. The reaction time and accuracy are included in each sheet.</p></disp-quote></p></sec><sec id="s0010"><label>2</label><title>Experimental design, materials and methods</title><sec id="s0015"><label>2.1</label><title>Materials and methods</title><p>Three colours (blue, green, and red) were assigned to three people (friend, self, and stranger) and mounted on the T-shirt of an avatar. The avatar subtended a visual angle of 5&#x000b0;&#x000d7;4.28&#x000b0; it was presented above a white fixation cross (0.8&#x000b0;&#x000d7;0.8&#x000b0;) at the centre of the screen. A label &#x02018;You&#x02019;, &#x02018;Friend&#x02019;, or &#x02018;Stranger&#x02019; (1.76&#x000b0;/2.52&#x000b0;&#x000d7;1.76&#x000b0; of visual angle) was displayed below the fixation of cross. The distance between fixation and both the centre of the avatar and the label was 2.9&#x000b0;. An avatar was presented either facing the participants (third-person perspective) or with its back to the participants (first-person perspective). Stimuli were shown on a grey background. Participants were not informed about the avatars in order to assess the implicit effect of embodied sensory processing on biases to the self and other people. The task was to judge whether the colour of the avatar&#x005f3;s T-shirt and the label matched their original assignments or not <xref rid="bib1" ref-type="bibr">[1]</xref>.</p><p>Participants were first instructed to associate three colours to people &#x02013; one to the self, one to a named best friend, and one to a stranger. The particular combinations of colours and labels were counterbalanced across participants. For example, a participant was told that blue represents your best friend &#x02013; Mary; you are green; and red represented a stranger. The avatar was not presented during the instruction. This took about 1-min. After this participants carried out a colour-label matching task, where they judged whether pairings of the colour and the label matched or not. Each trial started with the presentation of a 500&#x000a0;ms central fixation cross, followed by the pairing of a colour and label for 100&#x000a0;ms. Half the pairings conformed to the instruction (match trials), and the other pairs had re-combined colours and labels (mismatch trials). The order of the combinations (which colour was paired with which label) on mismatch trials was counterbalanced across labels; for example, green (friend) was re-paired with either of two mismatched labels &#x02018;You&#x02019; and &#x02018;Stranger&#x02019;. The next frame was a blank with a range of 800&#x02013;1200&#x000a0;ms (to capture the response). Participants were encouraged to press one of the two responses keys as quickly and accurately as possible within this last timeframe. Subsequently, written feedback (correct, incorrect, or slow!) was given in the centre of the screen for 500&#x000a0;ms. Participants were also informed of their overall accuracy performance at the end of each blocks.</p></sec><sec id="s0020"><label>2.2</label><title>Design and analyses</title><p>Separate data analyses were conducted for the match and mismatch trials (organised according to the colour of the stimulus) as the different responses were made to these two types of stimuli. We conducted repeated-measures ANOVAs with two within-subject factors &#x02013; association (self, friend, or stranger) and perspective (first- vs. third-person perspective). There was no trade-off between reaction times (RTs) and accuracy performance for any condition. We reported data on RTs and d prime respectively. We calculated the d prime in terms of the hits (&#x02018;yes&#x02019; responses to colour-label match pairs) and false alarms (&#x02018;yes&#x02019; responses to colour-label mismatch pairs, based on the colour assigned to the avatar), in order to calculate the sensitivity to the target colours. We tested if there is an enhanced sensitivity to the colour being associated with the self. Holm-Bonferroni corrections for <italic>&#x003b1;</italic>=0.05 were applied to all multiple comparisons.</p></sec></sec></body><back><ref-list><title>Reference</title><ref id="bib1"><label>1</label><element-citation publication-type="journal" id="sbref1"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>Y.</given-names></name><name><surname>Fuentes</surname><given-names>L.J.</given-names></name><name><surname>Humphreys</surname><given-names>G.W.</given-names></name><name><surname>Sui</surname><given-names>J.</given-names></name></person-group><article-title>Try to see it my way: embodied perspective enhances self and friend-biases in perceptual matching</article-title><source>Cognition</source><volume>153</volume><year>2016</year><fpage>108</fpage><lpage>117</lpage><pub-id pub-id-type="pmid">27183397</pub-id></element-citation></ref></ref-list><sec id="s0030" sec-type="supplementary-material"><label>Transparency document</label><title>Supplementary material</title><p><supplementary-material content-type="local-data" id="ec0005"><caption><p>Supplementary material</p></caption><media xlink:href="mmc1.docx"/></supplementary-material></p></sec><sec id="s0040" sec-type="supplementary-material"><label>Appendix A</label><title>Supplementary material</title><p><supplementary-material content-type="local-data" id="ec0010"><caption><p>Supplementary material</p></caption><media xlink:href="mmc2.xlsx"/></supplementary-material></p></sec><ack id="ack0005"><title>Acknowledgements</title><p>This work was supported by the <funding-source id="gs3">National Natural Science Foundation of China</funding-source> (Project 31371017) and <funding-source id="gs4">Tsinghua University Initiative Scientific Research Programme</funding-source> (Project 20131089329).</p></ack><fn-group><fn id="s0025" fn-type="supplementary-material"><label>Transparency document</label><p>Transparency data associated with this article can be found in the online version at <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.dib.2016.06.062" id="ir0005">doi:10.1016/j.dib.2016.06.062</ext-link>.</p></fn><fn id="s0035" fn-type="supplementary-material"><label>Appendix A</label><p>Supplementary data associated with this article can be found in the online version at <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.dib.2016.06.062" id="ir0010">doi:10.1016/j.dib.2016.06.062</ext-link>.</p></fn></fn-group></back></article>