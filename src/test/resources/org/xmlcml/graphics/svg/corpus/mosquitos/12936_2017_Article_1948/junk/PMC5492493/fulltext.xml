<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 1.1?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">5492493</article-id><article-id pub-id-type="doi">10.3390/s17061207</article-id><article-id pub-id-type="publisher-id">sensors-17-01207</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Traffic Sign Detection System for Locating Road Intersections and Roundabouts: The Chilean Case</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Villal&#x000f3;n-Sep&#x000fa;lveda</surname><given-names>Gabriel</given-names></name><xref ref-type="aff" rid="af1-sensors-17-01207">1</xref></contrib><contrib contrib-type="author"><name><surname>Torres-Torriti</surname><given-names>Miguel</given-names></name><xref ref-type="aff" rid="af1-sensors-17-01207">1</xref></contrib><contrib contrib-type="author"><name><surname>Flores-Calero</surname><given-names>Marco</given-names></name><xref ref-type="aff" rid="af2-sensors-17-01207">2</xref><xref ref-type="aff" rid="af3-sensors-17-01207">3</xref><xref rid="c1-sensors-17-01207" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Yang</surname><given-names>Simon X.</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-17-01207"><label>1</label>Departamento de Ingenier&#x000ed;a El&#x000e9;ctrica, Pontificia Universidad Cat&#x000f3;lica de Chile, Vicu&#x000f1;a Mackenna 4860, Casilla 306-22, Santiago, Chile; <email>procesosestocasticosespe@gmail.com</email> (G.V.-S.); <email>mtorrest@ing.puc.cl</email> (M.T.-T.)</aff><aff id="af2-sensors-17-01207"><label>2</label>Departamento de El&#x000e9;ctrica y Electr&#x000f3;nica, Universidad de las Fuerzas Armadas-ESPE, Av. Gral. Rumi&#x000f1;ahui s/n, PBX 171-5-231B Sangolqu&#x000ed;, Pichincha, Ecuador</aff><aff id="af3-sensors-17-01207"><label>3</label>Departamento de Sistemas Inteligentes, Tecnolog&#x000ed;as <italic>I</italic>&#x00026;<italic>H</italic>, CP 050102, Latacunga, Cotopaxi, Ecuador</aff><author-notes><corresp id="c1-sensors-17-01207"><label>*</label>Correspondence: <email>mjflores@espe.edu.ec</email>; Tel.: +593-23-989400 (ext. 1850)</corresp></author-notes><pub-date pub-type="epub"><day>25</day><month>5</month><year>2017</year></pub-date><pub-date pub-type="collection"><month>6</month><year>2017</year></pub-date><volume>17</volume><issue>6</issue><elocation-id>1207</elocation-id><history><date date-type="received"><day>02</day><month>1</month><year>2017</year></date><date date-type="accepted"><day>22</day><month>5</month><year>2017</year></date></history><permissions><copyright-statement>&#x000a9; 2017 by the authors.</copyright-statement><copyright-year>2017</copyright-year><license license-type="open-access"><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>This paper presents a traffic sign detection method for signs close to road intersections and roundabouts, such as stop and yield (give way) signs. The proposed method relies on statistical templates built using color information for both segmentation and classification. The segmentation method uses the RGB-normalized (ErEgEb) color space for ROIs (Regions of Interest) generation based on a chromaticity filter, where templates at 10 scales are applied to the entire image. Templates consider the mean and standard deviation of normalized color of the traffic signs to build thresholding intervals where the expected color should lie for a given sign. The classification stage employs the information of the statistical templates over YCbCr and ErEgEb color spaces, for which the background has been previously removed by using a probability function that models the probability that the pixel corresponds to a sign given its chromaticity values. This work includes an analysis of the detection rate as a function of the distance between the vehicle and the sign. Such information is useful to validate the robustness of the approach and is often not included in the existing literature. The detection rates, as a function of distance, are compared to those of the well-known Viola&#x02013;Jones method. The results show that for distances less than 48 m, the proposed method achieves a detection rate of <inline-formula><mml:math id="mm1"><mml:mrow><mml:mrow><mml:mn>87.5</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm2"><mml:mrow><mml:mrow><mml:mn>95.4</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for yield and stop signs, respectively. For distances less than 30 m, the detection rate is <inline-formula><mml:math id="mm3"><mml:mrow><mml:mrow><mml:mn>100</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for both signs. The Viola&#x02013;Jones approach has detection rates below <inline-formula><mml:math id="mm4"><mml:mrow><mml:mrow><mml:mn>20</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for distances between 30 and 48 m, and barely improves in the 20&#x02013;30 m range with detection rates of up to <inline-formula><mml:math id="mm5"><mml:mrow><mml:mrow><mml:mn>60</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Thus, the proposed method provides a robust alternative for intersection detection that relies on statistical color-based templates instead of shape information. The experiments employed videos of traffic signs taken in several streets of Santiago, Chile, using a research platform implemented at the Robotics and Automation Laboratory of PUC to develop driver assistance systems.</p></abstract><kwd-group><kwd>statistical template</kwd><kwd>traffic signs</kwd><kwd>color</kwd><kwd>road intersection</kwd><kwd>roundabouts</kwd><kwd>accidents</kwd></kwd-group></article-meta></front><body><sec id="sec1-sensors-17-01207"><title>1. Introduction</title><p>Traffic accidents are the primary cause of death for young people between 15 and 29 years old. Between 20 to 50 million people are injured each year, while <inline-formula><mml:math id="mm6"><mml:mrow><mml:mrow><mml:mn>1.3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> million died due to traffic accidents, of which <inline-formula><mml:math id="mm7"><mml:mrow><mml:mrow><mml:mn>91</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> take place in low and medium income countries [<xref rid="B1-sensors-17-01207" ref-type="bibr">1</xref>,<xref rid="B2-sensors-17-01207" ref-type="bibr">2</xref>,<xref rid="B3-sensors-17-01207" ref-type="bibr">3</xref>]. Latin America is a region with high rates of road traffic accidents [<xref rid="B2-sensors-17-01207" ref-type="bibr">2</xref>,<xref rid="B4-sensors-17-01207" ref-type="bibr">4</xref>], due to diverse reasons which include driver education and behavior, law enforcement, and lack of adequate road infrastructure. However, technology can also play an important role in driver assistance systems that contribute to the alertness of the driver and better driving behaviors [<xref rid="B5-sensors-17-01207" ref-type="bibr">5</xref>,<xref rid="B6-sensors-17-01207" ref-type="bibr">6</xref>].</p><p>Most traffic accidents occur in urban areas, especially at road intersections and roundabouts. Country statistics for traffic accidents show that a significant number happened at road intersections, for instance, <inline-formula><mml:math id="mm8"><mml:mrow><mml:mrow><mml:mn>22</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in the USA [<xref rid="B7-sensors-17-01207" ref-type="bibr">7</xref>,<xref rid="B8-sensors-17-01207" ref-type="bibr">8</xref>], <inline-formula><mml:math id="mm9"><mml:mrow><mml:mrow><mml:mn>58.7</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in Japan during 1995 [<xref rid="B9-sensors-17-01207" ref-type="bibr">9</xref>], <inline-formula><mml:math id="mm10"><mml:mrow><mml:mrow><mml:mn>13.75</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in Ecuador during 2015 [<xref rid="B10-sensors-17-01207" ref-type="bibr">10</xref>], and <inline-formula><mml:math id="mm11"><mml:mrow><mml:mrow><mml:mn>9.22</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in Chile during 2014 [<xref rid="B11-sensors-17-01207" ref-type="bibr">11</xref>]. Thus, the importance of developing systems for road intersections detection [<xref rid="B12-sensors-17-01207" ref-type="bibr">12</xref>], which unlike other aspects such as pedestrian detection, lane-tracking and driver drowsiness or distraction [<xref rid="B5-sensors-17-01207" ref-type="bibr">5</xref>,<xref rid="B6-sensors-17-01207" ref-type="bibr">6</xref>] has not received enough attention. Prior work has focused on pavement segmentation to detect intersections [<xref rid="B6-sensors-17-01207" ref-type="bibr">6</xref>] by analyzing the continuity and curvature of the road boundaries. However, occlusions in urban environments make the analysis of edges difficult or impossible. Therefore, the implementation of an advanced driving assistance system (ADAS) [<xref rid="B13-sensors-17-01207" ref-type="bibr">13</xref>] requires a module capable of detecting road signs in general, and specifically those found at intersections.</p><p>We propose a traffic sign detection approach based on statistical templates built using normalized color information. The novelty of the approach lies in the probabilistic model of the sign (or object) conditioned over the intensity of the normalized color channels instead of using traditional shape descriptors. The results show that this approach is robust to variations in distance between the car and the traffic sign, as well as variation in illumination. Unlike deep learning techniques, the results show that it is possible to implement the proposed traffic sign detection approach with small datasets of a few hundred images.</p><p>This paper is organized as follows. First, the state-of-the-art of traffic sign detection algorithms is discussed in <xref ref-type="sec" rid="sec2-sensors-17-01207">Section 2</xref>. <xref ref-type="sec" rid="sec3-sensors-17-01207">Section 3</xref> describes a new system for traffic sign detection and its modules based on color information. The experimental results, where an analysis between the detection rate and the distance is performed to verify the quality of this system, are presented in <xref ref-type="sec" rid="sec4-sensors-17-01207">Section 4</xref>. Finally, <xref ref-type="sec" rid="sec5-sensors-17-01207">Section 5</xref> is devoted to the conclusions and discussion of future work.</p></sec><sec id="sec2-sensors-17-01207"><title>2. State-of-the-Art</title><p>Traffic sign detection using visible-spectrum cameras may take different approaches. Some works implement feature classifiers. This means that a sliding-window method is used to compute features on different overlapping regions, which are then fed to the previously trained classifier [<xref rid="B14-sensors-17-01207" ref-type="bibr">14</xref>,<xref rid="B15-sensors-17-01207" ref-type="bibr">15</xref>]. The drawback of this strategy is that many positions and scales have to be tested using classifiers that may need computationally demanding training phases. More recent methods formulate a two-stage strategy, in which candidate or proposal regions are computed first by some &#x0201c;class-agnostic&#x0201d; segmentation process, i.e., extracting groups of pixels that share some characteristic without necessarily identifying whether they truly belong to the same class of object. In a second stage, some classification or decision process is used to complete the detection deciding whether some classes of objects sought are present or not. The methods proposed in [<xref rid="B16-sensors-17-01207" ref-type="bibr">16</xref>,<xref rid="B17-sensors-17-01207" ref-type="bibr">17</xref>,<xref rid="B18-sensors-17-01207" ref-type="bibr">18</xref>,<xref rid="B19-sensors-17-01207" ref-type="bibr">19</xref>,<xref rid="B20-sensors-17-01207" ref-type="bibr">20</xref>,<xref rid="B21-sensors-17-01207" ref-type="bibr">21</xref>,<xref rid="B22-sensors-17-01207" ref-type="bibr">22</xref>,<xref rid="B23-sensors-17-01207" ref-type="bibr">23</xref>,<xref rid="B24-sensors-17-01207" ref-type="bibr">24</xref>] can be found among recent approaches for traffic sign detection employing regional proposal strategies together with classifiers. The most recent approaches to segmentation and classification employed in traffic sign detection are discussed next.</p><sec id="sec2dot1-sensors-17-01207"><title>2.1. Segmentation for ROI Generation</title><p>In the context of traffic sign detection, blob generation and color analysis are the main techniques employed to segment regions of interest. Special efforts have been placed on making the color-based segmentation robust to large variations in illumination and weather conditions. Greenhalgh et al. [<xref rid="B16-sensors-17-01207" ref-type="bibr">16</xref>] transform RGB into grayscale images using the red and the blue components and experimentally obtained thresholds to generate ROIs. Salti et al. [<xref rid="B17-sensors-17-01207" ref-type="bibr">17</xref>] employ three color spaces derived from the RGB, the first to highlight road signs with a predominance of blue and red colors, the second one is for signs with intense red and the third one for the bright blue. Li et al. [<xref rid="B18-sensors-17-01207" ref-type="bibr">18</xref>] have built the Gaussian space (<inline-formula><mml:math id="mm12"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mi>&#x003bb;</mml:mi></mml:msub><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>&#x003bb;</mml:mi><mml:mi>&#x003bb;</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>), where objects dominated by the green-red and blue-yellow colors are highlighted. The preselected regions are in turn transformed to normalized values <inline-formula><mml:math id="mm13"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>&#x003bb;</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>&#x003bb;</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm14"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>&#x003bb;</mml:mi><mml:mi>&#x003bb;</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>&#x003bb;</mml:mi><mml:mi>&#x003bb;</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, which are fed to a <italic>k</italic>-means clustering [<xref rid="B25-sensors-17-01207" ref-type="bibr">25</xref>] to generate the ROIs. Zaklouta et al. [<xref rid="B19-sensors-17-01207" ref-type="bibr">19</xref>] implement two RGB-based chromatic filters for ROIS generation, one for signs that have a red color prevalence, and another filter for red-yellow predominance; in both cases, thresholds are defined in terms of mean and variance. Lillo et al. [<xref rid="B26-sensors-17-01207" ref-type="bibr">26</xref>] have used the L<inline-formula><mml:math id="mm15"><mml:mrow><mml:msup><mml:mrow/><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>a<inline-formula><mml:math id="mm16"><mml:mrow><mml:msup><mml:mrow/><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>b<inline-formula><mml:math id="mm17"><mml:mrow><mml:msup><mml:mrow/><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> space to detect signs in which the blue, green, yellow and red colors dominate. Based on the <italic>k</italic>-means clustering algorithm, the authors build a classifier that employs the <inline-formula><mml:math id="mm18"><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm19"><mml:mrow><mml:msup><mml:mi>b</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> components. Fleyeh et al. [<xref rid="B23-sensors-17-01207" ref-type="bibr">23</xref>] use the <italic>H</italic> and <italic>S</italic> components of the HSV space to train a classifier and implement the color segmentation that yields ROIs. More recently, Han et al. [<xref rid="B24-sensors-17-01207" ref-type="bibr">24</xref>] have used the <italic>H</italic> component of the <inline-formula><mml:math id="mm20"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> space, in which the traffic signs are highlighted in order to build a grayscale image where a set of ROIs is generated. Keser et al. [<xref rid="B27-sensors-17-01207" ref-type="bibr">27</xref>] have used the HSV filter intervals to generate a set of ROIs. Finally, Zhu et al. [<xref rid="B28-sensors-17-01207" ref-type="bibr">28</xref>] employ three different object proposal strategies (Selective Search, Edge Boxes and BING) and convolutional neural networks for classification, achieving an accuracy of 88% on average.</p></sec><sec id="sec2dot2-sensors-17-01207"><title>2.2. Recognition</title><p>The recognition stage typically employs feature classifiers, and therefore requires a feature descriptor and a classification algorithm. One of the most popular feature descriptors is the histogram of oriented gradients (HOG) [<xref rid="B15-sensors-17-01207" ref-type="bibr">15</xref>], which provides information about objects&#x02019; shape. Recent works in traffic sign detection that employ the HOG descriptor include [<xref rid="B16-sensors-17-01207" ref-type="bibr">16</xref>,<xref rid="B17-sensors-17-01207" ref-type="bibr">17</xref>,<xref rid="B19-sensors-17-01207" ref-type="bibr">19</xref>,<xref rid="B29-sensors-17-01207" ref-type="bibr">29</xref>]. Li et al. [<xref rid="B18-sensors-17-01207" ref-type="bibr">18</xref>] employ the PHOG descriptor, a variant of the HOG descriptor. Other descriptors are based upon the discrete Fourier transform [<xref rid="B26-sensors-17-01207" ref-type="bibr">26</xref>], the Hough transform [<xref rid="B23-sensors-17-01207" ref-type="bibr">23</xref>], the SURF method [<xref rid="B30-sensors-17-01207" ref-type="bibr">30</xref>], the values of the neighboring pixels in a ROI [<xref rid="B31-sensors-17-01207" ref-type="bibr">31</xref>], or predefined contour descriptors for basic shapes (circular, triangular, or rectangular) [<xref rid="B27-sensors-17-01207" ref-type="bibr">27</xref>].</p><p>Concerning classifiers, most of the recent work in traffic sign detection employs SVM (support vector machine) classifiers [<xref rid="B25-sensors-17-01207" ref-type="bibr">25</xref>]; see for example [<xref rid="B16-sensors-17-01207" ref-type="bibr">16</xref>,<xref rid="B17-sensors-17-01207" ref-type="bibr">17</xref>,<xref rid="B18-sensors-17-01207" ref-type="bibr">18</xref>,<xref rid="B19-sensors-17-01207" ref-type="bibr">19</xref>,<xref rid="B26-sensors-17-01207" ref-type="bibr">26</xref>]. Another popular classification approach relies on artificial neural networks (NN). For example, recent work by Huang et al. [<xref rid="B29-sensors-17-01207" ref-type="bibr">29</xref>] combines an NN-classifier with ELM (Extreme Learning Machine), and P&#x000e9;rez et al. [<xref rid="B32-sensors-17-01207" ref-type="bibr">32</xref>] relies on MLPs (MultiLayer Perceptrons). The simpler <italic>k</italic>-NN (<italic>k</italic>-nearest neighbors) algorithm [<xref rid="B25-sensors-17-01207" ref-type="bibr">25</xref>] is employed in the traffic sign detection method proposed in [<xref rid="B24-sensors-17-01207" ref-type="bibr">24</xref>]. Recently, Deep Learning techniques are being used for simultaneous detection and recognition of traffic signs. CNN (Convolutional Neural Networks) is also employed in many of the most recent papers&#x02014;Lau et al. [<xref rid="B31-sensors-17-01207" ref-type="bibr">31</xref>], Jung et al. [<xref rid="B33-sensors-17-01207" ref-type="bibr">33</xref>], Zeng et al. [<xref rid="B34-sensors-17-01207" ref-type="bibr">34</xref>], Zhu et al. [<xref rid="B28-sensors-17-01207" ref-type="bibr">28</xref>]&#x02014;which propose new architectures for automatic sign detection. Other strategies, such as the one employed by Li and Yang [<xref rid="B35-sensors-17-01207" ref-type="bibr">35</xref>] rely on a combination of DBM (Deep Boltzmann Machine) and CCA (Canonical Correlation Analysis) for feature extraction and classification. Lau et al. [<xref rid="B31-sensors-17-01207" ref-type="bibr">31</xref>] have also experimented with <inline-formula><mml:math id="mm21"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>B</mml:mi><mml:mi>N</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (Radial Basis Neural Networks) for classification.</p></sec><sec id="sec2dot3-sensors-17-01207"><title>2.3. Databases</title><p>The main traffic sign databases correspond to the following countries: Germany [<xref rid="B17-sensors-17-01207" ref-type="bibr">17</xref>,<xref rid="B19-sensors-17-01207" ref-type="bibr">19</xref>,<xref rid="B20-sensors-17-01207" ref-type="bibr">20</xref>,<xref rid="B29-sensors-17-01207" ref-type="bibr">29</xref>,<xref rid="B32-sensors-17-01207" ref-type="bibr">32</xref>], United Kingdom [<xref rid="B16-sensors-17-01207" ref-type="bibr">16</xref>], Spain [<xref rid="B22-sensors-17-01207" ref-type="bibr">22</xref>,<xref rid="B26-sensors-17-01207" ref-type="bibr">26</xref>], Japan [<xref rid="B36-sensors-17-01207" ref-type="bibr">36</xref>], China [<xref rid="B28-sensors-17-01207" ref-type="bibr">28</xref>] or Malaysia [<xref rid="B31-sensors-17-01207" ref-type="bibr">31</xref>]. Each country has its own regulations and standards concerning traffic signs, divided in regulatory, prevention and information categories [<xref rid="B17-sensors-17-01207" ref-type="bibr">17</xref>,<xref rid="B23-sensors-17-01207" ref-type="bibr">23</xref>,<xref rid="B26-sensors-17-01207" ref-type="bibr">26</xref>,<xref rid="B28-sensors-17-01207" ref-type="bibr">28</xref>,<xref rid="B36-sensors-17-01207" ref-type="bibr">36</xref>]. Generally, they do not follow the Vienna Convention-Complaint for traffic signs [<xref rid="B37-sensors-17-01207" ref-type="bibr">37</xref>].</p><p>Thus country-specific databases are required for the development of traffic sign detection systems. However, there is a lack of databases with traffic signs in Latin America. Therefore, another goal of this work is to contribute to the development of traffic sign detection systems that can be validated also on traffic signs of the Latin American region.</p></sec></sec><sec id="sec3-sensors-17-01207"><title>3. Proposed Approach for Segmentation and Recognition of Traffic Signs at Road Intersections and Roundabouts</title><p>The proposed computational strategy for detecting signs found at road intersections and roundabouts is composed of two parts. The first part generates ROIs in which traffic signs could be found calculating and analyzing color statistics in the normalized RGB space. The second part solves the recognition of signs in ROIs using a statistical template matching strategy using templates also in the normalized RGB space.</p><p>The detection must be done at the furthest possible distance, so that the driver has enough time to react and to stop in time. Examples of typical testing scenarios for the proposed approach are presented in <xref ref-type="fig" rid="sensors-17-01207-f001">Figure 1</xref>, which shows a distant stop sign and a yield (give way) sign.</p><sec id="sec3dot1-sensors-17-01207"><title>3.1. Chromaticity Filter for the Selection of ROIs</title><p>Under adequate illumination conditions, such as daylight or artificial lighting, the color of traffic signs is a feature that can be used to generate ROIs. Comparing the histograms of traffic signs in four color spaces, RGB, HSV, YCbCr and ErEgEb (the normalized RGB space) [<xref rid="B38-sensors-17-01207" ref-type="bibr">38</xref>,<xref rid="B39-sensors-17-01207" ref-type="bibr">39</xref>], it possible to observe in <xref ref-type="fig" rid="sensors-17-01207-f002">Figure 2</xref> that some of the color spaces provide better discrimination capability between traffic signs and the image backgrounds. In <xref ref-type="fig" rid="sensors-17-01207-f002">Figure 2</xref>, the histograms labeled RPOS correspond to histograms of the stop sign, while the curves labeled RNEG correspond to the histograms of backgrounds or scenes that do not contain traffic signs.</p><p>The color space that shows the smaller overlap between the histograms of positives (signs) and negatives (non-sign) is the ErEgEb space, where in particular the <inline-formula><mml:math id="mm22"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> channel shows a clear separation between the two classes. The <inline-formula><mml:math id="mm23"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> channel histograms have a small intersection, but likewise it serves to discard a significant portion of negatives. Finally, the <inline-formula><mml:math id="mm24"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> does not provide considerable information, but will be considered a part of the classification strategy, see <xref ref-type="fig" rid="sensors-17-01207-f002">Figure 2</xref>j&#x02013;l. A similar analysis was conducted for the yield sign and the results allow to conclude that the ErEgEb space, and in particular <inline-formula><mml:math id="mm25"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm26"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> channels provide a better discrimination capability.</p><p>Therefore, the candidate regions of interest can be detected using a chromaticity filter, i.e., a filter that works on the variables that define color hue (dominant wavelength) and color purity or saturation (difference between the intensity of the dominant wavelength with respect to white, grey or black) regardless of luminance (magnitude of the color components vector) or psychological perception of illumination brightness or intensity (as an average of the components of the color vector). In other words, a chromaticity filter only requires two variables that describe dominant wavelengths regardless of the total energy by mapping the components of the thrichromacy color model into a subspace of two normalized values. Assuming a normal distribution of the chromaticity channels <inline-formula><mml:math id="mm27"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm28"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, the selection thresholds for extracting ROIs can be defined as intervals <inline-formula><mml:math id="mm29"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm30"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm31"><mml:mrow><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm32"><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the mean and standard deviation of the channel <italic>c</italic> computed over a set of positives (images with traffic signs) according to:
<disp-formula id="FD1-sensors-17-01207"><label>(1a)</label><mml:math id="mm33"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mi>p</mml:mi></mml:msub><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>p</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD2-sensors-17-01207"><label>(1b)</label><mml:math id="mm34"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD3-sensors-17-01207"><label>(1c)</label><mml:math id="mm35"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm36"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>p</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the value of channel <italic>c</italic> at the pixel location <italic>p</italic> within the traffic sign of the <italic>i</italic>-th reference image (positive), <inline-formula><mml:math id="mm37"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>&#x02026;</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm38"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the number of positive images, <inline-formula><mml:math id="mm39"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the number of pixels within the reference traffic sign area, <inline-formula><mml:math id="mm40"><mml:mrow><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the mean value of channel <italic>c</italic> for the <italic>i</italic>-th image, <inline-formula><mml:math id="mm41"><mml:mrow><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is mean value of channel <italic>c</italic> across the ensemble of <inline-formula><mml:math id="mm42"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> images, and <inline-formula><mml:math id="mm43"><mml:mrow><mml:msubsup><mml:mi>&#x003bc;</mml:mi><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> is the variance of the mean values <inline-formula><mml:math id="mm44"><mml:mrow><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm45"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>&#x02026;</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. The value <inline-formula><mml:math id="mm46"><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:math></inline-formula> is set to minimize false positives and false negatives that will be passed to the recognition stage. A practical value that ensures the lowest amount of false positives while preventing misdetections was found to be <inline-formula><mml:math id="mm47"><mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. It is to be noted that using the so-called summed integral area tables or integral images [<xref rid="B14-sensors-17-01207" ref-type="bibr">14</xref>,<xref rid="B40-sensors-17-01207" ref-type="bibr">40</xref>] is possible to reduce the computation time of the average values on <inline-formula><mml:math id="mm48"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> sliding blocks. Two examples showing the generation of preliminary candidates for ROIs using windows sizes of <inline-formula><mml:math id="mm49"><mml:mrow><mml:mrow><mml:mn>50</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm50"><mml:mrow><mml:mrow><mml:mn>10</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> pixels are shown in <xref ref-type="fig" rid="sensors-17-01207-f003">Figure 3</xref> for values of <inline-formula><mml:math id="mm51"><mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>The last step for the final proposal of ROIs is to eliminate the overpopulation of candidates. To this end, all the candidates that are contained within or are a sub-window of another candidate are discarded, so that only the largest block remains. Next, windows whose mean value is closest to <inline-formula><mml:math id="mm52"><mml:mrow><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> in each neighborhood are selected so that there is only one candidate per neighborhood. <xref ref-type="fig" rid="sensors-17-01207-f004">Figure 4</xref> shows the final ROI proposal obtained using 10 window sizes ranging from <inline-formula><mml:math id="mm53"><mml:mrow><mml:mrow><mml:mn>10</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm54"><mml:mrow><mml:mrow><mml:mn>50</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> in geometric progression, with a fixed scaling factor between each size. The number of preliminary candidates satisfying the chromaticity filter threshold was <inline-formula><mml:math id="mm55"><mml:mrow><mml:mrow><mml:mn>32</mml:mn><mml:mo>,</mml:mo><mml:mn>849</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. This number is reduced to only 9 after the pre-candidate selection and merging step.</p></sec><sec id="sec3dot2-sensors-17-01207"><title>3.2. Recognition of Traffic Signs Based on Statistical Templates</title><p>The second stage of the proposed traffic sign detection approach is responsible for solving the identification of ROIs as traffic signs of a given type. To this end, a set of images is employed to create two statistical models, one with the mean intensity and the other with its standard deviation for each pixel belonging to the traffic sign. Testing on a sliding block for the percentage of pixels that fall within the expected intensity range for a given location provides a discriminator to detect traffic signs from background and non-traffic sign objects. A flow diagram of the proposed method is shown in <xref ref-type="fig" rid="sensors-17-01207-f005">Figure 5</xref>. The corresponding pseudocode of the algorithm is presented in Algorithm 1. The most effective recognition of traffic signs is achieved applying the algorithm to the Er and Eb channels of the ErEgEb color space. Only two channels conveying the chromaticity information are sufficient because the magnitude normalization satisfies <inline-formula><mml:math id="mm56"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mi>E</mml:mi><mml:mi>g</mml:mi><mml:mo>+</mml:mo><mml:mi>E</mml:mi><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, making the third channel dependent on the other two (<inline-formula><mml:math id="mm57"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>E</mml:mi><mml:mi>r</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>E</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>). The probabilistic model that defines the discrimination thresholds is discussed next.</p><p>In order to develop the probabilistic template matching model to recognize traffic signs, it is first convenient to introduce the following notation:
<list list-type="bullet"><list-item><p><inline-formula><mml:math id="mm58"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>&#x0225c;</mml:mo><mml:mfenced separators="" open="{" close="}"><mml:mi>I</mml:mi><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>..</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is a block or subwindow composed of pixel values <inline-formula><mml:math id="mm59"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>,</p></list-item><list-item><p><inline-formula><mml:math id="mm60"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a vector with the pixel chromaticity components Er and Eb, <inline-formula><mml:math id="mm61"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>..</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>,</p></list-item><list-item><p><inline-formula><mml:math id="mm62"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo>&#x0225c;</mml:mo><mml:mfenced separators="" open="{" close="}"><mml:mi>O</mml:mi><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>..</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is the object (sign) class label of subwindow <italic>I</italic>,</p></list-item><list-item><p><inline-formula><mml:math id="mm63"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>: the object (sign) class label at every pixel <italic>k</italic>, <inline-formula><mml:math id="mm64"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>..</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> in the subwindow <italic>I</italic>.</p></list-item></list></p><p>The probability that a window corresponds to a particular object (sign) is:
<disp-formula id="FD4-sensors-17-01207"><label>(2)</label><mml:math id="mm65"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>O</mml:mi><mml:mo>&#x02223;</mml:mo><mml:mi>I</mml:mi></mml:mfenced></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>P</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mover><mml:mi>O</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>&#x02223;</mml:mo><mml:mi>I</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mn>2</mml:mn><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>...</mml:mo><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>n</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:munderover><mml:mo>&#x0220f;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover><mml:mi>O</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mrow><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#x02223;</mml:mo><mml:mi>I</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:munderover><mml:mo>&#x0220f;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mfenced separators="" open="[" close="]"><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>P</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>O</mml:mi><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mi>I</mml:mi><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mfenced></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm66"><mml:mrow><mml:mover><mml:mi>O</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is the complement of <italic>O</italic> and <inline-formula><mml:math id="mm67"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover><mml:mi>O</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mrow><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#x02223;</mml:mo><mml:mi>I</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>..</mml:mo><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>n</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover><mml:mi>O</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mrow><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#x02223;</mml:mo><mml:mi>I</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is obtained by assuming independence between <inline-formula><mml:math id="mm68"><mml:mrow><mml:mrow><mml:mover><mml:mi>O</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mrow><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm69"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>[</mml:mo><mml:mi>j</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, for all <inline-formula><mml:math id="mm70"><mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#x02260;</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. This is possible in view of the fact that <italic>O</italic> and <italic>I</italic> are random samples [<xref rid="B41-sensors-17-01207" ref-type="bibr">41</xref>,<xref rid="B42-sensors-17-01207" ref-type="bibr">42</xref>]. Also, this means that the probability of a pixel not belonging to an object only depends on the pixel value and not its neighborhood. In other words, the background (non-sign) pixels are assumed to be conditionally independent with respect to their neighborhood. This assumption is not entirely true in every area of the background, but simplifies the probability computation.</p><array orientation="portrait"><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 1:</bold> Traffic sign recognition algorithm based on statistical templates</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Input</bold>: <inline-formula><mml:math id="mm71"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>: candidate image block, <break/><inline-formula><mml:math id="mm72"><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:math></inline-formula>: pixel acceptance amplitude parameter,<break/><inline-formula><mml:math id="mm73"><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mi>&#x003c3;</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>: background pixels discard threshold, <break/><inline-formula><mml:math id="mm74"><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>: minimal amount of pixels threshold for detection.<break/><bold>Output</bold>: <inline-formula><mml:math id="mm75"><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>: binary detection output.<break/><monospace>// Loading pre-trained masks</monospace><break/><inline-formula><mml:math id="mm78"><mml:mrow><mml:mrow><mml:msub><mml:mover><mml:mi>A</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mi>M</mml:mi></mml:msub><mml:mo>=</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
<monospace>LoadAverageMask()</monospace>;<break/><inline-formula><mml:math id="mm79"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>=</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
<monospace>LoadStandardDeviationMask()</monospace>;<break/><monospace>// Pixel mask discarding corresponding to the background</monospace><break/><inline-formula><mml:math id="mm82"><mml:mrow><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mi>Y</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mi>&#x003c3;</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>;<break/><monospace>// Minimum and maximum accepted masks</monospace><break/><inline-formula><mml:math id="mm85"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mover><mml:mi>A</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mi>M</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>;<break/><inline-formula><mml:math id="mm86"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mover><mml:mi>A</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mi>M</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>;<break/><monospace>// Pixel mask accepted</monospace><break/><inline-formula><mml:math id="mm89"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mi>N</mml:mi><mml:mo>&#x02264;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>&#x02264;</mml:mo><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>;<break/><monospace>// Final decision</monospace><break/><bold>if</bold>
<monospace>SumPixels</monospace>(<inline-formula><mml:math id="mm92"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>)/<monospace>SumPixels</monospace>(<inline-formula><mml:math id="mm93"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) <inline-formula><mml:math id="mm94"><mml:mrow><mml:mrow><mml:mo>&#x02265;</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>
<bold>then</bold><break/>&#x02003;&#x02003;<inline-formula><mml:math id="mm96"><mml:mrow><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>;<break/><bold>else</bold><break/>&#x02003;&#x02003;<inline-formula><mml:math id="mm99"><mml:mrow><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>;<break/><bold>end</bold></td></tr></tbody></array><p>In order to compute the posterior probability <inline-formula><mml:math id="mm101"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>O</mml:mi><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mi>I</mml:mi><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> that a pixel <italic>k</italic> has label <inline-formula><mml:math id="mm102"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> given its chromaticity value <inline-formula><mml:math id="mm103"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, Bayes&#x02019; theorem is used
<disp-formula id="FD5-sensors-17-01207"><label>(3)</label><mml:math id="mm104"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>O</mml:mi><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mi>I</mml:mi><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mfenced><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>I</mml:mi><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mi>O</mml:mi><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mfenced><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>O</mml:mi><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mfenced></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>I</mml:mi><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
to express the posterior probability in terms of the measurement model <inline-formula><mml:math id="mm105"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>I</mml:mi><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mi>O</mml:mi><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>The measurement model <inline-formula><mml:math id="mm106"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>I</mml:mi><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo><mml:mo>|</mml:mo><mml:mi>O</mml:mi><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> can be obtained assuming that the pixel values of the object (sign) of interest follow a normal distribution <inline-formula><mml:math id="mm107"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm108"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, with mean chromaticity <inline-formula><mml:math id="mm109"><mml:mrow><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and standard deviation <inline-formula><mml:math id="mm110"><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> obtained from a set of reference images, see the last row of <xref ref-type="fig" rid="sensors-17-01207-f002">Figure 2</xref>.</p><p>The likelihood or conditional probability that the measured chromaticity values <inline-formula><mml:math id="mm111"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mi>r</mml:mi><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mi>g</mml:mi><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> at pixel <italic>k</italic> take some value in the interval <inline-formula><mml:math id="mm112"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>E</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm113"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, given object class, is then given by:
<disp-formula id="FD6-sensors-17-01207"><label>(4)</label><mml:math id="mm114"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>I</mml:mi><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mi>r</mml:mi><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mi>b</mml:mi><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo><mml:mo>)</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mi>O</mml:mi><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:munder><mml:mo>&#x0220f;</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mfrac><mml:mn>2</mml:mn><mml:msqrt><mml:mi>&#x003c0;</mml:mi></mml:msqrt></mml:mfrac><mml:munderover><mml:mo>&#x0222b;</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mrow></mml:mfrac></mml:munderover><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:munder><mml:mo>&#x0220f;</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mi>erf</mml:mi><mml:mfenced open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mrow></mml:mfrac></mml:mstyle></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>The prior probability <inline-formula><mml:math id="mm115"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>O</mml:mi><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> of finding the object (sign) in an image can be obtained experimentally from the set of reference images as:
<disp-formula id="FD7-sensors-17-01207"><label>(5)</label><mml:math id="mm116"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>O</mml:mi><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm117"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the true positive rate and <inline-formula><mml:math id="mm118"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the false positive rate for the object (sign) of class (type) <italic>O</italic>. This ratio is known as a positive predictive value and it describes the probability of traffic signs being correctly detected [<xref rid="B43-sensors-17-01207" ref-type="bibr">43</xref>].</p><p>Finally, the probability of the occurrence of chromaticity values <inline-formula><mml:math id="mm119"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>I</mml:mi><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mi>b</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> can be obtained empirically or analytically. The empirical approach would require constructing the histograms for the Er and Eb channels using a representative set of training data and normalizing the histograms to obtain the ratio between the number of pixels with the given chromaticity levels with respect to the the total amount of pixels. Analytically, a cumulative density function for the values of each channel can be deduced under the assumption that RGB values distribute according to a uniform distribution within a window block that contains both object and background pixels (see appendix for calculations details). The cumulative function for the chromaticity values under the uniform distribution assumption is given by:
<disp-formula id="FD8-sensors-17-01207"><label>(6)</label><mml:math id="mm120"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>3</mml:mn></mml:mfrac></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>21</mml:mn><mml:msup><mml:mi>y</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mn>27</mml:mn><mml:msup><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&#x02212;</mml:mo><mml:mn>9</mml:mn><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>6</mml:mn><mml:msup><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>3</mml:mn></mml:mfrac><mml:mo>&#x0003c;</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>5</mml:mn><mml:msup><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi>y</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>6</mml:mn><mml:msup><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>&#x0003c;</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#x02265;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The probability density function <inline-formula><mml:math id="mm121"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> associated to the cumulative distribution of the chromaticity values <inline-formula><mml:math id="mm122"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mi>b</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is easily obtained from (6) by deriving <inline-formula><mml:math id="mm123"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with respect to <italic>y</italic>. <xref ref-type="fig" rid="sensors-17-01207-f006">Figure 6</xref> depicts the cumulative and density functions. The density function <inline-formula><mml:math id="mm124"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> reaches a maximum at <inline-formula><mml:math id="mm125"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#x0223c;</mml:mo><mml:mn>0.36</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, which is the most likely background value without prior knowledge of the object (sign) class. Thus, it is desirable that the objects of interest have their chromaticity levels far away from this value.</p><p>The analysis thus far provides enough tools to compute the probability that a window corresponds to the object of interest using (2) and the statistical templates for the mean and standard deviation <inline-formula><mml:math id="mm126"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> over the window block <inline-formula><mml:math id="mm127"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>&#x02026;</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. However, these templates consider both the pixels of the object of interest and the background; therefore, to improve discrimination, it is convenient to discard background pixels.</p><p>To this end, <xref ref-type="fig" rid="sensors-17-01207-f007">Figure 7</xref> presents the representative points to the comparison of histograms of the pixels corresponding to the background and to the object of interest presented in <xref ref-type="fig" rid="sensors-17-01207-f008">Figure 8</xref> reveals that the luminance channel Y spreads over the entire range of possible values for background objects. Hence, the standard deviation of the luminance channel is a good indicator to determine whether the pixel is part of the object of interest or part of the background. <xref ref-type="fig" rid="sensors-17-01207-f009">Figure 9</xref> shows the mean and standard deviation for the luminance channel Y. It is clear that the template built using the variance provides higher contrast between background and foreground than using the mean value of the luminance to create a mask for discarding background regions.</p><p>To create a mask for discarding background pixels, an adequate thresholding value for the standard deviation template is <inline-formula><mml:math id="mm128"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> as may be observed in <xref ref-type="fig" rid="sensors-17-01207-f010">Figure 10</xref> since it allows to retain most of the pixels of the object of interest and discard all of the background. Assuming the luminance Y would distribute Gaussianly, <inline-formula><mml:math id="mm129"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo>&#x0003e;</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> would imply that <inline-formula><mml:math id="mm130"><mml:mrow><mml:mrow><mml:mn>95</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of the samples would fall within <inline-formula><mml:math id="mm131"><mml:mrow><mml:mrow><mml:mo>&#x000b1;</mml:mo><mml:mn>120</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> intensity levels, thus it would cover a range of 240 levels, which would be almost the full range for an 8-bit image with 255 levels. Thus, higher values for the threshold on <inline-formula><mml:math id="mm132"><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mi>Y</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are not convenient, while lower values cause part of the object to be labeled as background, as shown in <xref ref-type="fig" rid="sensors-17-01207-f010">Figure 10</xref>a with <inline-formula><mml:math id="mm133"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>55</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>By the foregoing, the channels to be used in the recognition stage have the luminance to remove the background and the chromaticity values Er and Eb to confirm the ROIs are of a given traffic sign, ensuring robustness to illumination variations.</p></sec></sec><sec id="sec4-sensors-17-01207"><title>4. Testing Methodology and Experimental Results</title><sec id="sec4dot1-sensors-17-01207"><title>4.1. Perception and Processing Systems</title><p>The vehicle shown in <xref ref-type="fig" rid="sensors-17-01207-f011">Figure 11</xref>a was employed as a testing platform for the experiments. The system comprises several cameras (visible spectrum, IR, catadioptric), an IMU and a RTK GPS; see [<xref rid="B6-sensors-17-01207" ref-type="bibr">6</xref>] for further details. For the experiments presented here, only one camera from Imaging Source, model DFK31BF03 model (see <xref ref-type="fig" rid="sensors-17-01207-f011">Figure 11</xref>b) was used together with the RTK GPS from Navcom Technology, model SF-2050, with decimeter positioning accuracy (see <xref ref-type="fig" rid="sensors-17-01207-f011">Figure 11</xref>c). The camera has a resolution of 1024 &#x000d7; 768 pixels and delivers images at 30 fps, while the GPS sampling rate is 10 Hz. Thus, the GPS data was interpolated to match the sampling instants of the camera. Registering the position is important in order to compute detection rates as a distance function. The processing of the images was carried out on a PC with an Intel Core 2 Duo processor with a 2.0 GHz frequency, and 3.5 GB of RAM. All the algorithms were implemented in C++ using the OpenCV version 2.2 library.</p></sec><sec id="sec4dot2-sensors-17-01207"><title>4.2. Training and Validation Dataset</title><p>The training dataset contains 2567 negative examples, 122 images containing stops signs and 80 images containing yield signs. The positive samples were randomly rotated, scaled and translated in order to produce 7000 positive examples; see <xref ref-type="fig" rid="sensors-17-01207-f012">Figure 12</xref>. The validation dataset contains 273 stop signs and 447 yield signs captured in six different driving runs.</p><p>The datasets consider a sequence of signs as the car approaches different intersections in the city of Santiago, Chile. Both datasets contain traffic signs in real driving conditions, under varying illumination and partial occlusion.</p><p>The database has been made available at RAL [<xref rid="B44-sensors-17-01207" ref-type="bibr">44</xref>] to contribute to the making of new studies and the development of ADAS.</p></sec><sec id="sec4dot3-sensors-17-01207"><title>4.3. Experiments Employing the Viola&#x02013;Jones Method and the Proposed Statistical Template Approach</title><p>In this section, the proposed traffic sign detection approach based on statistical templates is tested and compared to the well-know Viola&#x02013;Jones method [<xref rid="B45-sensors-17-01207" ref-type="bibr">45</xref>].</p><sec id="sec4dot3dot1-sensors-17-01207"><title>4.3.1. Viola&#x02013;Jones Method:</title><p>The detection rates of the Viola&#x02013;Jones approach for the stop and yield signs are summarized in <xref ref-type="table" rid="sensors-17-01207-t001">Table 1</xref> as a function of distance. The detection rate for stop signs is <inline-formula><mml:math id="mm134"><mml:mrow><mml:mrow><mml:mn>100</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> when distances are 20 m or less. However, the detection rate rapidly falls to <inline-formula><mml:math id="mm135"><mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for distances above 35 m. On the other hand, less that <inline-formula><mml:math id="mm136"><mml:mrow><mml:mrow><mml:mn>3.1</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of the yield signs was detected for distances below 20 m. For distances above 20 m, it was not possible to detect any yield sign. This is attributed, in part, to the fact that some samples in the dataset contained signs with marks and graffiti on it. These results show that the Viola&#x02013;Jones approach is highly sensitive to possible modifications in the signs sought. The false alarm rates of the Viola&#x02013;Jones approach for the stop and yield signs were practically <inline-formula><mml:math id="mm137"><mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, as shown in <xref ref-type="table" rid="sensors-17-01207-t002">Table 2</xref>.</p></sec><sec id="sec4dot3dot2-sensors-17-01207"><title>4.3.2. Statistical Template Method:</title><p>The proposed algorithm was executed employing eight different templates sized from <inline-formula><mml:math id="mm138"><mml:mrow><mml:mrow><mml:mn>14</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>14</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm139"><mml:mrow><mml:mrow><mml:mn>50</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> pixels in geometric progression. The detection rates presented in <xref ref-type="table" rid="sensors-17-01207-t003">Table 3</xref> show that a high rate of success was achieved for distances up to 37 m in the case of the stop sign and 35 in the case of the yield sign. Detection rates fall to <inline-formula><mml:math id="mm140"><mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for distances above 52 m in the case of the stop sign and 48 m in the case of the yield sign. Unlike the Viola&#x02013;Jones approach, false alarm rates were between <inline-formula><mml:math id="mm141"><mml:mrow><mml:mrow><mml:mn>3.6</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm142"><mml:mrow><mml:mrow><mml:mn>6.9</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as shown in <xref ref-type="table" rid="sensors-17-01207-t002">Table 2</xref>. A comparison of the detection rate performance of the proposed method with the Viola&#x02013;Jones approach is presented in <xref ref-type="fig" rid="sensors-17-01207-f013">Figure 13</xref>. This figure shows the effectiveness of the proposed approach at detecting traffic signs earlier than the Viola&#x02013;Jones method does.</p><p><xref ref-type="table" rid="sensors-17-01207-t001">Table 1</xref> and <xref ref-type="table" rid="sensors-17-01207-t003">Table 3</xref> were constructed with approximately 430 images for each distance class.</p><p>In terms of the computational effort, the Viola&#x02013;Jones method required 450 ms per frame while the proposed approach required 950 ms per frame. This amount could be decreased for real-time operation using dedicated graphic processing units (GPUs).</p><p>Finally, <xref ref-type="fig" rid="sensors-17-01207-f014">Figure 14</xref> presents an extended example of the proposed system, in real driving conditions during the day, for both stop and yield signs.</p></sec></sec></sec><sec id="sec5-sensors-17-01207"><title>5. Conclusions</title><p>This paper presented an approach based on statistical templates computed from chromaticity and luminance values for traffic sign detection near road intersections and roundabouts. The approach is evaluated using a dataset of stop and yield (give way) signs from Chile and compared to the well-known Viola&#x02013;Jones classification method.</p><p>The proposed approach is divided into two stages. The first stage is a segmentation stage based on the chromaticity filter applied to the Er and Eg channels of the ErEgEb color space (the normalized RGB color space). In the second stage, the chromaticity filter returns candidate regions that are responsible for the recognition of traffic signs and which must be tested. The selection of the Er and Eg channels is based on the analysis of the histograms of the color components in four color spaces (RGB, YCrCB, HSV and ErEgEb), which shows that the ErEgEb provides greater capacity to discriminate candidate traffic signs. The segmentation stage employs a selection threshold that can be computed automatically from a reference dataset. The recognition stage is based on a statistical template built with information of the <inline-formula><mml:math id="mm143"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm144"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> chromaticity channels and the <italic>Y</italic> luminance channel. The luminance channel is employed to create a traffic sign mask from the variance of pixels that allows background regions within a ROI to be discarded. The Er and Eb channels are used to compute a statistical template that provides the sign selection thresholds. A probabilistic model and probability distribution function were derived to construct a recognition process based on Bayesian inference.</p><p>The results obtained show that the proposed approach has higher detection rates than the Viola&#x02013;Jones method. The experiments considered the evaluation of the detection rate at different distances as the vehicle approaches a traffic sign. On average, the proposed approach exhibits a detection rate of <inline-formula><mml:math id="mm145"><mml:mrow><mml:mrow><mml:mn>87.5</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for yield signs and <inline-formula><mml:math id="mm146"><mml:mrow><mml:mrow><mml:mn>95.4</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for stop signs at distances below 48 m.</p><p>The two main advantages of the proposed approach are summarized in that it does not require a computationally expensive training or calibration stage, and it is not sensitive to changes in illumination, partial occlusion or marks drawn on the traffic sign. Future work will consider extending the proposed approach to the detection of traffic lights at road junctions and crosswalks, as well as to the detection of other signs not necessarily found at road intersections. Ongoing research is considering joint analysis of lane geometry analysis and edge continuity together with traffic sign detection to improve the detection of road intersections. The main limitation of this model is the computing time, which is about 950 ms per frame. As a future work, we will work to reduce this processing time.</p></sec></body><back><ack><title>Acknowledgments</title><p>This project has been supported by Comision Nacional de Ciencia y Tecnolog&#x000ed;a de Chile (Conicyt) Grant 11060251, Basal Project FB0008, and by Universidad de las Fuerzas Armadas ESPE through both Plan de Movilidad con Fines de Investigci&#x000f3;n 2015 Grant and <inline-formula><mml:math id="mm147"><mml:mrow><mml:mrow><mml:mn>2014</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>P</mml:mi><mml:mi>I</mml:mi><mml:mi>T</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>007</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> Research Project, and by Tecnolog&#x000ed;as I&#x00026;H.</p></ack><notes><title>Author Contributions</title><p>The framework was proposed by Miguel Torres, and further development and implementation were realized by Gabriel Villal&#x000f3;n. Marco Flores and Miguel Torres studied some of the ideas, analyzed the experiment results and prepared the manuscript.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflict of interest.</p></notes><app-group><app id="app1-sensors-17-01207"><title>Appendix A. Deduction of the Background Probability Distribution</title><p>To derive the cumulative density function of chromaticity values for any pixel <inline-formula><mml:math id="mm148"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in Equation (6), it is assumed that each channel of the RGB color spaces follows a uniform distribution. Thus, three random variables <inline-formula><mml:math id="mm149"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm150"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm151"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> representing each channel are defined according to the uniform distribution:
<disp-formula id="FD9-sensors-17-01207"><label>(A1)</label><mml:math id="mm152"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>&#x0223c;</mml:mo><mml:mi>U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.222222em"/><mml:mo>&#x021d2;</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>0</mml:mn><mml:mo>&#x02228;</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02265;</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>S</mml:mi></mml:mfrac></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The cumulative distribution function of the chromaticity channel <inline-formula><mml:math id="mm153"><mml:mrow><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>/</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, given by:
<disp-formula id="FD10-sensors-17-01207"><label>(A2)</label><mml:math id="mm154"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>Y</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>y</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#x02264;</mml:mo><mml:mi>y</mml:mi></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
is calculated as follows. The procedure for <inline-formula><mml:math id="mm155"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="mm156"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:msub><mml:mi>X</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> is exactly the same.</p><p>In finding a closed-form expression for (A2), the following cases are considered:
<list list-type="bullet"><list-item><p><underline>Case <inline-formula><mml:math id="mm157"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></underline>:
<disp-formula id="FD11-sensors-17-01207"><label>(A3)</label><mml:math id="mm158"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>F</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>P</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#x02264;</mml:mo><mml:mn>0</mml:mn></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>P</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x02264;</mml:mo><mml:mn>0</mml:mn></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p><underline>Case <inline-formula><mml:math id="mm159"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></underline>:
<disp-formula id="FD12-sensors-17-01207"><label>(A4)</label><mml:math id="mm160"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>F</mml:mi><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>P</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#x02264;</mml:mo><mml:mn>1</mml:mn></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>P</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>&#x02265;</mml:mo><mml:mn>0</mml:mn></mml:mfenced><mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p><underline>Case <inline-formula><mml:math id="mm161"><mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></underline>:
<disp-formula id="FD13-sensors-17-01207"><label>(A5)</label><mml:math id="mm162"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>F</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>P</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#x02264;</mml:mo><mml:mi>y</mml:mi></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>P</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x02264;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></list-item></list></p><p>The last case amounts to solving the following integral:
<disp-formula id="FD14-sensors-17-01207"><label>(A6)</label><mml:math id="mm163"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x0222b;</mml:mo><mml:mn>0</mml:mn><mml:mi>S</mml:mi></mml:munderover><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:munderover><mml:mo>&#x0222b;</mml:mo><mml:mn>0</mml:mn><mml:mi>S</mml:mi></mml:munderover><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:munderover><mml:mo>&#x0222b;</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mfrac><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munderover><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The inner integral of <inline-formula><mml:math id="mm164"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> must consider that if <inline-formula><mml:math id="mm165"><mml:mrow><mml:mrow><mml:mfrac><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02265;</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, then density function <inline-formula><mml:math id="mm166"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>S</mml:mi></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula> is integrated up to <italic>S</italic>, while if <inline-formula><mml:math id="mm167"><mml:mrow><mml:mrow><mml:mfrac><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, then the integral is computed for <inline-formula><mml:math id="mm168"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mfrac><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. For the latter to be fulfilled throughout the non-zero domain of <inline-formula><mml:math id="mm169"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm170"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the following must be satisfied:
<disp-formula id="FD15-sensors-17-01207"><label>(A7)</label><mml:math id="mm171"><mml:mrow><mml:mrow><mml:mfrac><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02264;</mml:mo><mml:mi>S</mml:mi><mml:mspace width="0.277778em"/><mml:mo>&#x02200;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>S</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#x021d2;</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>3</mml:mn></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Similarly, when analyzing the limits of integration for the integrals over <inline-formula><mml:math id="mm172"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm173"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, so that the non-null parts are integrated and the result is continuous over the range of integration, the following intervals are obtained for <italic>y</italic>: <inline-formula><mml:math id="mm174"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>3</mml:mn></mml:mfrac><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm175"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>3</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm176"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>For <inline-formula><mml:math id="mm177"><mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>3</mml:mn></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula>:
<disp-formula id="FD16-sensors-17-01207"><label>(A8)</label><mml:math id="mm178"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x0222b;</mml:mo><mml:mn>0</mml:mn><mml:mi>S</mml:mi></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>S</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x0222b;</mml:mo><mml:mn>0</mml:mn><mml:mi>S</mml:mi></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>S</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x0222b;</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mfrac><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>S</mml:mi></mml:mfrac></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>For <inline-formula><mml:math id="mm179"><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>3</mml:mn></mml:mfrac><mml:mo>&#x0003c;</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula>, the integral is calculated as follows:
<disp-formula id="FD17-sensors-17-01207"><label>(A9)</label><mml:math id="mm180"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>F</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x0222b;</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mi>y</mml:mi></mml:mfrac><mml:mi>S</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>S</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x0222b;</mml:mo><mml:mn>0</mml:mn><mml:mi>S</mml:mi></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>S</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x0222b;</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mfrac><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>S</mml:mi></mml:mfrac></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>+</mml:mo><mml:munderover><mml:mo>&#x0222b;</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mi>y</mml:mi></mml:mfrac><mml:mi>S</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mi>S</mml:mi></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>S</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x0222b;</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mi>y</mml:mi></mml:mfrac><mml:mi>S</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>S</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x0222b;</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mfrac><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>S</mml:mi></mml:mfrac></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>+</mml:mo><mml:munderover><mml:mo>&#x0222b;</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mi>y</mml:mi></mml:mfrac><mml:mi>S</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mi>S</mml:mi></mml:munderover><mml:mfrac><mml:mn>1</mml:mn><mml:mi>S</mml:mi></mml:mfrac><mml:munderover><mml:mo>&#x0222b;</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mi>y</mml:mi></mml:mfrac><mml:mi>S</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mi>S</mml:mi></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>S</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x0222b;</mml:mo><mml:mn>0</mml:mn><mml:mi>S</mml:mi></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>S</mml:mi></mml:mfrac></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>21</mml:mn><mml:msup><mml:mi>y</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mn>27</mml:mn><mml:msup><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&#x02212;</mml:mo><mml:mn>9</mml:mn><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>6</mml:mn><mml:msup><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Finally, for <inline-formula><mml:math id="mm181"><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>&#x0003c;</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, the integral is computed as follows:
<disp-formula id="FD18-sensors-17-01207"><label>(A10)</label><mml:math id="mm182"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>F</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x0222b;</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mi>y</mml:mi></mml:mfrac><mml:mi>S</mml:mi></mml:mrow></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>S</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x0222b;</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mi>y</mml:mi></mml:mfrac><mml:mi>S</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>S</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x0222b;</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mfrac><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>S</mml:mi></mml:mfrac></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>+</mml:mo><mml:munderover><mml:mo>&#x0222b;</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mi>y</mml:mi></mml:mfrac><mml:mi>S</mml:mi></mml:mrow></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>S</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x0222b;</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mi>y</mml:mi></mml:mfrac><mml:mi>S</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mi>S</mml:mi></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>S</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x0222b;</mml:mo><mml:mn>0</mml:mn><mml:mi>S</mml:mi></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>S</mml:mi></mml:mfrac></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>+</mml:mo><mml:munderover><mml:mo>&#x0222b;</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mi>y</mml:mi></mml:mfrac><mml:mi>S</mml:mi></mml:mrow><mml:mi>S</mml:mi></mml:munderover><mml:mfrac><mml:mn>1</mml:mn><mml:mi>S</mml:mi></mml:mfrac><mml:munderover><mml:mo>&#x0222b;</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mi>S</mml:mi></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>S</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x0222b;</mml:mo><mml:mn>0</mml:mn><mml:mi>S</mml:mi></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>S</mml:mi></mml:mfrac></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>5</mml:mn><mml:msup><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi>y</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>6</mml:mn><mml:msup><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>The above results in the cumulative distribution presented in Equation (6).</p></app><app id="app2-sensors-17-01207"><title>Appendix B. Traffic Sign Detection by Using the Viola&#x02013;Jones Method</title><p>The Viola&#x02013;Jones object recognition approach [<xref rid="B14-sensors-17-01207" ref-type="bibr">14</xref>,<xref rid="B40-sensors-17-01207" ref-type="bibr">40</xref>] has been used in multiple computer vision applications [<xref rid="B45-sensors-17-01207" ref-type="bibr">45</xref>,<xref rid="B46-sensors-17-01207" ref-type="bibr">46</xref>,<xref rid="B47-sensors-17-01207" ref-type="bibr">47</xref>]. In this work, it has been used to build a traffic signs detector, trained for stop and yield signs. <xref ref-type="fig" rid="sensors-17-01207-f015">Figure A1</xref> shows the first and second convolution masks based on Haar-like features for each of the traffic signs considered in this work.</p><fig id="sensors-17-01207-f015" orientation="portrait" position="anchor"><label>Figure A1</label><caption><p>First and second Haar-like convolution masks for stop and yield signs employed in this work.</p></caption><graphic xlink:href="sensors-17-01207-g015"/></fig></app></app-group><ref-list><title>References</title><ref id="B1-sensors-17-01207"><label>1.</label><element-citation publication-type="web"><person-group person-group-type="author"><collab>World Health Organization (WHO)</collab></person-group><article-title>Road Traffic Injuries</article-title><comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://www.who.int/violenceinjuryprevention/roadtraffic/en/">http://www.who.int/violenceinjuryprevention/roadtraffic/en/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2015-05-24">(accessed on 24 May 2015)</date-in-citation></element-citation></ref><ref id="B2-sensors-17-01207"><label>2.</label><element-citation publication-type="web"><person-group person-group-type="author"><collab>World Health Organization (WHO)</collab></person-group><article-title>La OMS y la FIA a&#x000fa;nan Esfuerzos Para Mejorar La Seguridad Vial</article-title><comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://www.who.int/mediacentre/news/releases/2003/pr11/es/">http://www.who.int/mediacentre/news/releases/2003/pr11/es/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2015-05-24">(accessed on 24 May 2015)</date-in-citation></element-citation></ref><ref id="B3-sensors-17-01207"><label>3.</label><element-citation publication-type="web"><person-group person-group-type="author"><collab>World Health Organization (WHO)</collab></person-group><article-title>Lesiones Causadas Por el Tr&#x000e1;nsito</article-title><comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://www.who.int/mediacentre/factsheets/fs358/es/">http://www.who.int/mediacentre/factsheets/fs358/es/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2015-05-24">(accessed on 24 May 2015)</date-in-citation></element-citation></ref><ref id="B4-sensors-17-01207"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fraser</surname><given-names>B.</given-names></name></person-group><article-title>Traffic accidents scar Latin America&#x02019;s roads</article-title><source>Lancet</source><year>2005</year><volume>366</volume><fpage>703</fpage><lpage>704</lpage><pub-id pub-id-type="doi">10.1016/S0140-6736(05)67158-9</pub-id><pub-id pub-id-type="pmid">16130228</pub-id></element-citation></ref><ref id="B5-sensors-17-01207"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jim&#x000e9;nez-Pinto</surname><given-names>J.</given-names></name><name><surname>Torres-Torriti</surname><given-names>M.</given-names></name></person-group><article-title>Optical Flow and Driver&#x02019;s Kinematics Analysis for State of Alert Sensing</article-title><source>Sensors</source><year>2013</year><volume>13</volume><fpage>4225</fpage><lpage>4257</lpage><pub-id pub-id-type="doi">10.3390/s130404225</pub-id><?supplied-pmid 23539029?><pub-id pub-id-type="pmid">23539029</pub-id></element-citation></ref><ref id="B6-sensors-17-01207"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tapia-Espinoza</surname><given-names>R.</given-names></name><name><surname>Torres-Torriti</surname><given-names>M.</given-names></name></person-group><article-title>Robust Lane Sensing and Departure Warning under Shadows and Occlusions</article-title><source>Sensors</source><year>2013</year><volume>13</volume><fpage>3270</fpage><lpage>3298</lpage><pub-id pub-id-type="doi">10.3390/s130303270</pub-id><?supplied-pmid 23478598?><pub-id pub-id-type="pmid">23478598</pub-id></element-citation></ref><ref id="B7-sensors-17-01207"><label>7.</label><element-citation publication-type="gov"><person-group person-group-type="author"><collab>National Highway Traffic Safety Administration (NHTSA)</collab></person-group><article-title>Traffic Fatalities Up Sharply in 2015</article-title><comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://www.nhtsa.gov/">http://www.nhtsa.gov/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2015-05-24">(accessed on 24 May 2015)</date-in-citation></element-citation></ref><ref id="B8-sensors-17-01207"><label>8.</label><element-citation publication-type="web"><person-group person-group-type="author"><collab>Mesriani Law Group</collab></person-group><article-title>Accidents Caused by Dangerous Intersections</article-title><comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://www.hg.org/article.asp?id=7652">http://www.hg.org/article.asp?id=7652</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2015-05-24">(accessed on 24 May 2015)</date-in-citation></element-citation></ref><ref id="B9-sensors-17-01207"><label>9.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y.</given-names></name><name><surname>Nihan</surname><given-names>N.L.</given-names></name></person-group><source>Quantitative Analysis on Angle-Accident Risk at Signalized Intersections</source><publisher-name>Research Associate Department of Civil Engineering University of Washington</publisher-name><publisher-loc>Seattle, WA, USA</publisher-loc><year>2015</year></element-citation></ref><ref id="B10-sensors-17-01207"><label>10.</label><element-citation publication-type="gov"><person-group person-group-type="author"><collab>Agencia Nacional de Tr&#x000e1;nsito del Ecuador</collab></person-group><article-title>Estad&#x000ed;sticas de Transporte Terrestre Y Seguridad Vial</article-title><comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://www.ant.gob.ec/">http://www.ant.gob.ec/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2015-05-24">(accessed on 24 May 2015)</date-in-citation></element-citation></ref><ref id="B11-sensors-17-01207"><label>11.</label><element-citation publication-type="gov"><person-group person-group-type="author"><collab>CONASET</collab></person-group><article-title>Observatorio de Datos De Accidentes</article-title><comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://estadconaset.mtt.gob.cl/">https://estadconaset.mtt.gob.cl/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2015-05-24">(accessed on 24 May 2015)</date-in-citation></element-citation></ref><ref id="B12-sensors-17-01207"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Nie</surname><given-names>Y.</given-names></name><name><surname>Chen</surname><given-names>Q.</given-names></name><name><surname>Chen</surname><given-names>T.</given-names></name><name><surname>Sun</surname><given-names>Z.</given-names></name><name><surname>Dai</surname><given-names>B.</given-names></name></person-group><article-title>Camera and lidar fusion for road intersection detection</article-title><source>Proceedings of the IEEE Symposium on Electrical and Electronics Engineering</source><conf-loc>Kuala Lumpur, Malaysia</conf-loc><conf-date>24&#x02013;27 June 2012</conf-date><fpage>273</fpage><lpage>276</lpage></element-citation></ref><ref id="B13-sensors-17-01207"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Horgan</surname><given-names>J.</given-names></name><name><surname>Hughes</surname><given-names>C.</given-names></name><name><surname>McDonald</surname><given-names>J.</given-names></name><name><surname>Yogamani</surname><given-names>S.</given-names></name></person-group><article-title>Vision-Based Driver Assistance Systems: Survey, Taxonomy and Advances</article-title><source>Proceedings of the IEEE 18th International Conference on Intelligent Transportation Systems (ITSC 2015)</source><conf-loc>Las Palmas de Gran Canaria, Spain</conf-loc><conf-date>15&#x02013;18 September 2015</conf-date><fpage>2032</fpage><lpage>2039</lpage></element-citation></ref><ref id="B14-sensors-17-01207"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Viola</surname><given-names>P.</given-names></name><name><surname>Jones</surname><given-names>M.</given-names></name></person-group><article-title>Rapid object detection using a boosted cascade of simple features</article-title><source>Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2001)</source><conf-loc>Kauai, HI, USA</conf-loc><conf-date>8&#x02013;14 December 2001</conf-date><volume>Volume 1</volume><fpage>I-511</fpage><lpage>I-518</lpage></element-citation></ref><ref id="B15-sensors-17-01207"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Dalal</surname><given-names>N.</given-names></name><name><surname>Triggs</surname><given-names>B.</given-names></name></person-group><article-title>Histograms of oriented gradients for human detection</article-title><source>Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&#x02019;05)</source><conf-loc>San Diego, CA, USA</conf-loc><conf-date>20&#x02013;25 June 2005</conf-date><volume>Volume 1</volume><fpage>886</fpage><lpage>893</lpage></element-citation></ref><ref id="B16-sensors-17-01207"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greenhalgh</surname><given-names>J.</given-names></name><name><surname>Mirmehdi</surname><given-names>M.</given-names></name></person-group><article-title>Real-Time Detection and Recognition of Road Traffic Signs</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2012</year><volume>13</volume><fpage>1498</fpage><lpage>1506</lpage><pub-id pub-id-type="doi">10.1109/TITS.2012.2208909</pub-id></element-citation></ref><ref id="B17-sensors-17-01207"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salti</surname><given-names>S.</given-names></name><name><surname>Petrelli</surname><given-names>A.</given-names></name><name><surname>Tombari</surname><given-names>F.</given-names></name><name><surname>Fioraio</surname><given-names>N.</given-names></name><name><surname>DiStefano</surname><given-names>L.</given-names></name></person-group><article-title>Traffic sign detection via interest region extraction</article-title><source>Pattern Recogn.</source><year>2015</year><volume>48</volume><fpage>1039</fpage><lpage>1049</lpage><pub-id pub-id-type="doi">10.1016/j.patcog.2014.05.017</pub-id></element-citation></ref><ref id="B18-sensors-17-01207"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>H.</given-names></name><name><surname>Sun</surname><given-names>F.</given-names></name><name><surname>Liu</surname><given-names>L.</given-names></name><name><surname>Wang</surname><given-names>L.</given-names></name></person-group><article-title>A novel traffic sign detection method via color segmentation and robust shape matching</article-title><source>Neurocomputing</source><year>2015</year><volume>169</volume><fpage>77</fpage><lpage>88</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2014.12.111</pub-id></element-citation></ref><ref id="B19-sensors-17-01207"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zaklouta</surname><given-names>F.</given-names></name><name><surname>Stanciulescu</surname><given-names>B.</given-names></name></person-group><article-title>Real-Time Traffic-Sign Recognition Using Tree Classifiers</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2012</year><volume>13</volume><fpage>1507</fpage><lpage>1514</lpage><pub-id pub-id-type="doi">10.1109/TITS.2012.2225618</pub-id></element-citation></ref><ref id="B20-sensors-17-01207"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zaklouta</surname><given-names>F.</given-names></name><name><surname>Stanciulescu</surname><given-names>B.</given-names></name></person-group><article-title>Real-time traffic sign recognition in three stages</article-title><source>Robot. Auton. Syst.</source><year>2014</year><volume>62</volume><fpage>16</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1016/j.robot.2012.07.019</pub-id></element-citation></ref><ref id="B21-sensors-17-01207"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mogelmose</surname><given-names>A.</given-names></name><name><surname>Trivedi</surname><given-names>M.M.</given-names></name><name><surname>Moeslund</surname><given-names>T.B.</given-names></name></person-group><article-title>Vision-Based Traffic Sign Detection and Analysis for Intelligent Driver Assistance Systems: Perspectives and Survey</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2012</year><volume>13</volume><fpage>1484</fpage><lpage>1497</lpage><pub-id pub-id-type="doi">10.1109/TITS.2012.2209421</pub-id></element-citation></ref><ref id="B22-sensors-17-01207"><label>22.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Carrasco</surname><given-names>J.</given-names></name></person-group><article-title>Advanced Driver Assistance System Based on Computer Vision Using Detection, Recognition and Tracking of Road Signs</article-title><source>Ph.D. Thesis</source><publisher-name>Laboratorio de Sistemas Inteligentes, Universidad Carlos III de Madrid</publisher-name><publisher-loc>Madrid, Spain</publisher-loc><year>2009</year></element-citation></ref><ref id="B23-sensors-17-01207"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Fleyeh</surname><given-names>H.</given-names></name><name><surname>Biswas</surname><given-names>R.</given-names></name><name><surname>Davami</surname><given-names>E.</given-names></name></person-group><article-title>Traffic sign detection based on AdaBoost color segmentation and SVM classification</article-title><source>Proceedings of the 2013 IEEE EUROCON</source><conf-loc>Zagreb, Croatia</conf-loc><conf-date>1&#x02013;4 July 2013</conf-date><fpage>2005</fpage><lpage>2010</lpage></element-citation></ref><ref id="B24-sensors-17-01207"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Han</surname><given-names>Y.</given-names></name><name><surname>Virupakshappa</surname><given-names>K.</given-names></name><name><surname>Oruklu</surname><given-names>E.</given-names></name></person-group><article-title>Robust traffic sign recognition with feature extraction and k-NN classification methods</article-title><source>Proceedings of the 2015 IEEE International Conference on Electro/Information Technology (EIT)</source><conf-loc>Dekalb, IL, USA</conf-loc><conf-date>21&#x02013;23 May 2015</conf-date><fpage>484</fpage><lpage>488</lpage></element-citation></ref><ref id="B25-sensors-17-01207"><label>25.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hastie</surname><given-names>T.</given-names></name><name><surname>Tibshirani</surname><given-names>R.</given-names></name><name><surname>Friedman</surname><given-names>J.</given-names></name></person-group><source>The Elements of Statistical Learning</source><edition>2nd ed.</edition><publisher-name>Springer</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2009</year></element-citation></ref><ref id="B26-sensors-17-01207"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lillo</surname><given-names>J.</given-names></name><name><surname>Mora</surname><given-names>I.</given-names></name><name><surname>Figuera</surname><given-names>C.</given-names></name><name><surname>Rojo</surname><given-names>J.L.</given-names></name></person-group><article-title>Traffic sign segmentation and classification using statistical learning methods</article-title><source>Neurocomputing</source><year>2015</year><volume>1</volume><fpage>286</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2014.11.026</pub-id></element-citation></ref><ref id="B27-sensors-17-01207"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Keser</surname><given-names>T.</given-names></name><name><surname>Kramar</surname><given-names>G.</given-names></name><name><surname>Nozica</surname><given-names>D.</given-names></name></person-group><article-title>Traffic Signs Shape Recognition Based on Contour Descriptor Analysis</article-title><source>Proceedings of the IEEE International Conference on Smart Systems and Technologies (SST)</source><conf-loc>Osijek, Croatia</conf-loc><conf-date>12&#x02013;14 October 2016</conf-date></element-citation></ref><ref id="B28-sensors-17-01207"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>Z.</given-names></name><name><surname>Liang</surname><given-names>D.</given-names></name><name><surname>Zhang</surname><given-names>S.</given-names></name><name><surname>Huang</surname><given-names>X.</given-names></name><name><surname>Li</surname><given-names>B.</given-names></name><name><surname>Hu</surname><given-names>S.</given-names></name></person-group><article-title>Traffic-Sign Detection and Classification in the Wild</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>27&#x02013;30 June 2016</conf-date></element-citation></ref><ref id="B29-sensors-17-01207"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>Z.</given-names></name><name><surname>Yu</surname><given-names>Y.</given-names></name><name><surname>Gu</surname><given-names>J.</given-names></name></person-group><article-title>A Novel Method for Traffic Sign Recognition based on Extreme Learning Machine</article-title><source>Proceedings of the IEEE 11th World Congress on Intelligent Control and Automation (WCICA)</source><conf-loc>Shenyang, China</conf-loc><conf-date>29 June&#x02013;4 July 2014</conf-date><fpage>1451</fpage><lpage>1456</lpage></element-citation></ref><ref id="B30-sensors-17-01207"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bay</surname><given-names>H.</given-names></name><name><surname>Ess</surname><given-names>A.</given-names></name><name><surname>Tuytelaars</surname><given-names>T.</given-names></name><name><surname>Gool</surname><given-names>L.V.</given-names></name></person-group><article-title>Speeded-Up Robust Features (SURF)</article-title><source>Comput. Vis. Image Underst.</source><year>2008</year><volume>110</volume><fpage>346</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1016/j.cviu.2007.09.014</pub-id></element-citation></ref><ref id="B31-sensors-17-01207"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lau</surname><given-names>M.M.</given-names></name><name><surname>Lim</surname><given-names>K.H.</given-names></name><name><surname>Gopalai</surname><given-names>A.A.</given-names></name></person-group><article-title>Malaysia Traffic Sign Recognitio on with Convolutional Neural Network</article-title><source>Proceedings of the IEEE International Conference on Digital Signal Processing (DSP)</source><conf-loc>Singapore</conf-loc><conf-date>21&#x02013;24 July 2015</conf-date><fpage>1006</fpage><lpage>1010</lpage></element-citation></ref><ref id="B32-sensors-17-01207"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Perez-Perez</surname><given-names>S.E.</given-names></name><name><surname>Gonzalez-Reyna</surname><given-names>S.E.</given-names></name><name><surname>Ledesma-Orozco</surname><given-names>S.E.</given-names></name><name><surname>Avina-Cervantes</surname><given-names>J.G.</given-names></name></person-group><article-title>Principal component analysis for speed limit Traffic Sign Recognition</article-title><source>Proceedings of the 2013 IEEE International Autumn Meeting on Power Electronics and Computing (ROPEC)</source><conf-loc>Morelia, Mexico</conf-loc><conf-date>13&#x02013;15 November 2013</conf-date><fpage>1</fpage><lpage>5</lpage></element-citation></ref><ref id="B33-sensors-17-01207"><label>33.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Jung</surname><given-names>S.</given-names></name><name><surname>Lee</surname><given-names>U.</given-names></name><name><surname>Jung</surname><given-names>J.</given-names></name><name><surname>Shim</surname><given-names>D.H.</given-names></name></person-group><article-title>Real-time Traffic Sign Recognition system with deep convolutional neural network</article-title><source>Proceedings of the Ubiquitous Robots and Ambient Intelligence (URAI)</source><conf-loc>Xian, China</conf-loc><conf-date>19&#x02013;22 August 2016</conf-date></element-citation></ref><ref id="B34-sensors-17-01207"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeng</surname><given-names>Y.</given-names></name><name><surname>Xu</surname><given-names>X.</given-names></name><name><surname>Shen</surname><given-names>D.</given-names></name><name><surname>Fang</surname><given-names>Y.</given-names></name><name><surname>Xiao</surname><given-names>Z.</given-names></name></person-group><article-title>Traffic Sign Recognition Using Kernel Extreme Learning Machines With Deep Perceptual Features</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2016</year><volume>PP</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1109/TITS.2016.2614916</pub-id><pub-id pub-id-type="pmid">27840592</pub-id></element-citation></ref><ref id="B35-sensors-17-01207"><label>35.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Li</surname><given-names>C.</given-names></name><name><surname>Yang</surname><given-names>C.</given-names></name></person-group><article-title>The research on traffic sign recognition based on deep learning</article-title><source>Proceedings of the 2016 16th IEEE International Symposium on Communications and Information Technologies (ISCIT)</source><conf-loc>Qingdao, China</conf-loc><conf-date>26&#x02013;28 September 2016</conf-date><fpage>156</fpage><lpage>161</lpage></element-citation></ref><ref id="B36-sensors-17-01207"><label>36.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Nguyen</surname><given-names>B.T.</given-names></name><name><surname>Shim</surname><given-names>J.</given-names></name><name><surname>Kim</surname><given-names>J.K.</given-names></name></person-group><article-title>Fast Traffic Sign Detection under Challenging Conditions</article-title><source>Proceedings of the 2014 International Conference on Audio, Language and Image Processing (ICALIP)</source><conf-loc>Shanghai, China</conf-loc><conf-date>7&#x02013;9 July 2014</conf-date><fpage>749</fpage><lpage>752</lpage></element-citation></ref><ref id="B37-sensors-17-01207"><label>37.</label><element-citation publication-type="web"><article-title>Convention on Road Signs and Signals</article-title><comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://www.unece.org/fileadmin/DAM/trans/conventn/signalse.pdf">http://www.unece.org/fileadmin/DAM/trans/conventn/signalse.pdf</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2015-05-24">(accessed on 24 May 2015)</date-in-citation></element-citation></ref><ref id="B38-sensors-17-01207"><label>38.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Flores</surname><given-names>M.</given-names></name><name><surname>Armingol</surname><given-names>M.</given-names></name><name><surname>Escalera de la</surname><given-names>A.</given-names></name></person-group><article-title>New probability models for face detection and tracking in color images</article-title><source>Proceedings of the 2007 IEEE International Symposium on Intelligent Signal Processing (WISP 2007)</source><conf-loc>Alcala de Henares, Spain</conf-loc><conf-date>3&#x02013;5 October 2007</conf-date><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B39-sensors-17-01207"><label>39.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jain</surname><given-names>A.K.</given-names></name><name><surname>Li</surname><given-names>S.Z.</given-names></name></person-group><source>Handbook of Face Recognition</source><publisher-name>Springer-Verlag Inc.</publisher-name><publisher-loc>Secaucus, NJ, USA</publisher-loc><year>2005</year><fpage>117</fpage></element-citation></ref><ref id="B40-sensors-17-01207"><label>40.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Viola</surname><given-names>P.</given-names></name><name><surname>Jones</surname><given-names>M.</given-names></name></person-group><article-title>Robust real-time face detection</article-title><source>Proceedings of the Eighth IEEE International Conference on Computer Vision (ICCV 2001)</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>7&#x02013;14 July 2001</conf-date></element-citation></ref><ref id="B41-sensors-17-01207"><label>41.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Duda</surname><given-names>R.</given-names></name><name><surname>Hart</surname><given-names>P.</given-names></name><name><surname>Stork</surname><given-names>D.</given-names></name></person-group><source>Pattern Classification</source><publisher-name>Wiley</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2001</year><fpage>42</fpage></element-citation></ref><ref id="B42-sensors-17-01207"><label>42.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Casella</surname><given-names>G.</given-names></name><name><surname>Berger</surname><given-names>R.</given-names></name></person-group><source>Statistical Inference</source><publisher-name>Duxbury</publisher-name><publisher-loc>Pacific Grove, CA, USA</publisher-loc><year>2002</year><fpage>207</fpage></element-citation></ref><ref id="B43-sensors-17-01207"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heston</surname><given-names>T.F.</given-names></name></person-group><article-title>Standardizing predictive values in diagnostic imaging research</article-title><source>J. Magn. Reson. Imaging</source><year>2011</year><volume>2</volume><fpage>506</fpage><lpage>507</lpage><pub-id pub-id-type="doi">10.1002/jmri.22466</pub-id><?supplied-pmid 21274995?><pub-id pub-id-type="pmid">21274995</pub-id></element-citation></ref><ref id="B44-sensors-17-01207"><label>44.</label><element-citation publication-type="web"><article-title>Robotics and Automation Laboratory, School of Engineering, Pontificia Universidad Cat&#x000f3;lica de Chile</article-title><comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://ral.ing.puc.cl/datasets/intersection/index.htm">http://ral.ing.puc.cl/datasets/intersection/index.htm</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2015-05-24">(accessed on 24 May 2015)</date-in-citation></element-citation></ref><ref id="B45-sensors-17-01207"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Viola</surname><given-names>P.A.</given-names></name><name><surname>Jones</surname><given-names>M.J.</given-names></name><name><surname>Snow</surname><given-names>D.</given-names></name></person-group><article-title>Detecting pedestrians using patterns of motion and appearance</article-title><source>Int. J. Comput. Vis.</source><year>2005</year><volume>63</volume><fpage>153</fpage><lpage>161</lpage><pub-id pub-id-type="doi">10.1007/s11263-005-6644-8</pub-id></element-citation></ref><ref id="B46-sensors-17-01207"><label>46.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Flores</surname><given-names>M.</given-names></name></person-group><article-title>Sistema Avanzado de Asistencia a la Conducci&#x000f3;n Mediante Visi&#x000f3;n por Computador para la Detecci&#x000f3;n de la Somnolencia</article-title><source>Ph.D. Thesis</source><publisher-name>Laboratorio de Sistemas Inteligentes, Universidad Carlos III de Madrid</publisher-name><publisher-loc>Madrid, Spain</publisher-loc><year>2009</year></element-citation></ref><ref id="B47-sensors-17-01207"><label>47.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Houben</surname><given-names>S.</given-names></name><name><surname>Stallkamp</surname><given-names>J.</given-names></name><name><surname>Salmen</surname><given-names>J.</given-names></name><name><surname>Schlipsing</surname><given-names>M.</given-names></name><name><surname>Igel</surname><given-names>C.</given-names></name></person-group><article-title>Detection of Traffic Signs in Real-World Images: The German Traffic Sign Detection Benchmark</article-title><source>Proceedings of the IEEE International Joint Conference on Neural Networks (IJCNN)</source><conf-loc>Dallas, TX, USA</conf-loc><conf-date>4&#x02013;9 August 2013</conf-date><fpage>205</fpage><lpage>216</lpage></element-citation></ref></ref-list></back><floats-group><fig id="sensors-17-01207-f001" orientation="portrait" position="float"><label>Figure 1</label><caption><p>Examples of testing scenarios showing stop (<bold>Left</bold>) and yield (<bold>Right</bold>) signs near a road intersection and a roundabout.</p></caption><graphic xlink:href="sensors-17-01207-g001"/></fig><fig id="sensors-17-01207-f002" orientation="portrait" position="float"><label>Figure 2</label><caption><p>Histograms of the average Stop sign images (<italic>c</italic>POS) and their background (<italic>c</italic>NEG) for each channel <italic>c</italic> in the RGB, YCrCb, HSV and ErEgEb color spaces.</p></caption><graphic xlink:href="sensors-17-01207-g002a"/><graphic xlink:href="sensors-17-01207-g002b"/></fig><fig id="sensors-17-01207-f003" orientation="portrait" position="float"><label>Figure 3</label><caption><p>Examples of ROI (regions of interest) candidates generated for the stop sign using the chromaticity filter before region merging.</p></caption><graphic xlink:href="sensors-17-01207-g003"/></fig><fig id="sensors-17-01207-f004" orientation="portrait" position="float"><label>Figure 4</label><caption><p>Example of ROIs obtained after removal/merging of overlapping pre-candidates selected with the chromaticity filter.</p></caption><graphic xlink:href="sensors-17-01207-g004"/></fig><fig id="sensors-17-01207-f005" orientation="portrait" position="float"><label>Figure 5</label><caption><p>Flow chart of the recognition stage based on statistical templates.</p></caption><graphic xlink:href="sensors-17-01207-g005"/></fig><fig id="sensors-17-01207-f006" orientation="portrait" position="float"><label>Figure 6</label><caption><p>Background probability model, cumulative density function <inline-formula><mml:math id="mm183"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<bold>a</bold>), and probability density function <inline-formula><mml:math id="mm184"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<bold>b</bold>), for <inline-formula><mml:math id="mm185"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>r</mml:mi><mml:mi>E</mml:mi><mml:mi>g</mml:mi><mml:mi>E</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> space, considering that each channel of RGB space follows a uniform distribution.</p></caption><graphic xlink:href="sensors-17-01207-g006"/></fig><fig id="sensors-17-01207-f007" orientation="portrait" position="float"><label>Figure 7</label><caption><p>Representative points of a stop sign template obtained by averaging reference samples in the <inline-formula><mml:math id="mm186"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>r</mml:mi><mml:mi>E</mml:mi><mml:mi>g</mml:mi><mml:mi>E</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> space.</p></caption><graphic xlink:href="sensors-17-01207-g007"/></fig><fig id="sensors-17-01207-f008" orientation="portrait" position="float"><label>Figure 8</label><caption><p>Histograms in three color spaces for pixels in the reference areas of <xref ref-type="fig" rid="sensors-17-01207-f007">Figure 7</xref>.</p></caption><graphic xlink:href="sensors-17-01207-g008a"/><graphic xlink:href="sensors-17-01207-g008b"/></fig><fig id="sensors-17-01207-f009" orientation="portrait" position="float"><label>Figure 9</label><caption><p>Scaled images of the mean <inline-formula><mml:math id="mm187"><mml:mrow><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>Y</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (<bold>a</bold>) and standard deviation <inline-formula><mml:math id="mm188"><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mi>Y</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (<bold>b</bold>) of the Y channel.</p></caption><graphic xlink:href="sensors-17-01207-g009"/></fig><fig id="sensors-17-01207-f010" orientation="portrait" position="float"><label>Figure 10</label><caption><p>Discarding the background in the Y channel with different thresholds: (<bold>a</bold>) <inline-formula><mml:math id="mm189"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>55</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>; (<bold>b</bold>) <inline-formula><mml:math id="mm190"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mspace width="3.33333pt"/><mml:mo>=</mml:mo><mml:mspace width="3.33333pt"/><mml:mn>60</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>; and (<bold>c</bold>) <inline-formula><mml:math id="mm191"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>65</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></caption><graphic xlink:href="sensors-17-01207-g010"/></fig><fig id="sensors-17-01207-f011" orientation="portrait" position="float"><label>Figure 11</label><caption><p>Experimental platform, vehicle (<bold>a</bold>); camera (<bold>b</bold>); and GPS (<bold>c</bold>).</p></caption><graphic xlink:href="sensors-17-01207-g011"/></fig><fig id="sensors-17-01207-f012" orientation="portrait" position="float"><label>Figure 12</label><caption><p>Typical traffic signs of road intersections and roundabouts in Chile: stop and yield signs; under different lighting conditions (sunny (<bold>a</bold>,<bold>b</bold>), normal (<bold>c</bold>,<bold>d</bold>) and dark (<bold>e</bold>,<bold>f</bold>)) and observer positions.</p></caption><graphic xlink:href="sensors-17-01207-g012"/></fig><fig id="sensors-17-01207-f013" orientation="portrait" position="float"><label>Figure 13</label><caption><p>Comparison of detection rates versus distance between the Viola&#x02013;Jones method and the statistical templates method for the stop and yield signs.</p></caption><graphic xlink:href="sensors-17-01207-g013"/></fig><fig id="sensors-17-01207-f014" orientation="portrait" position="float"><label>Figure 14</label><caption><p>Example of the proposed system in different instants of time, in daytime conditions. Stop sign (<bold>Top</bold>) and Yield sign (<bold>Bottom</bold>), where blue indicates the ROIs and red shows the true sign.</p></caption><graphic xlink:href="sensors-17-01207-g014"/></fig><table-wrap id="sensors-17-01207-t001" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-17-01207-t001_Table 1</object-id><label>Table 1</label><caption><p>Detection rate based on the Viola&#x02013;Jones method.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Distance to the Intersection [mt]</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Yield %</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Stop %</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#x0003e;62</td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm192"><mml:mrow><mml:mrow><mml:mn>0.0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm193"><mml:mrow><mml:mrow><mml:mn>0.0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">62&#x02013;55</td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm194"><mml:mrow><mml:mrow><mml:mn>0.0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm195"><mml:mrow><mml:mrow><mml:mn>0.0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">55&#x02013;48</td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm196"><mml:mrow><mml:mrow><mml:mn>0.0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm197"><mml:mrow><mml:mrow><mml:mn>0.0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">48&#x02013;41</td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm198"><mml:mrow><mml:mrow><mml:mn>0.0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm199"><mml:mrow><mml:mrow><mml:mn>0.0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">41&#x02013;34</td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm200"><mml:mrow><mml:mrow><mml:mn>0.0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm201"><mml:mrow><mml:mrow><mml:mn>6.5</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">34&#x02013;27</td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm202"><mml:mrow><mml:mrow><mml:mn>0.0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm203"><mml:mrow><mml:mrow><mml:mn>21.0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">27&#x02013;20</td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm204"><mml:mrow><mml:mrow><mml:mn>0.0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm205"><mml:mrow><mml:mrow><mml:mn>57.6</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0003c;20</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm206"><mml:mrow><mml:mrow><mml:mn>3.1</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm207"><mml:mrow><mml:mrow><mml:mn>100.0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr></tbody></table></table-wrap><table-wrap id="sensors-17-01207-t002" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-17-01207-t002_Table 2</object-id><label>Table 2</label><caption><p>False alarm rate per frame.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Yield</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Stop</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Viola&#x02013;Jones</td><td align="center" valign="middle" rowspan="1" colspan="1">0.006</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Statistical template</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.036</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.069</td></tr></tbody></table></table-wrap><table-wrap id="sensors-17-01207-t003" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-17-01207-t003_Table 3</object-id><label>Table 3</label><caption><p>Detection rate based on the statistical template method.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Distance to the Intersection [mt]</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Yield %</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Stop %</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#x0003e;62</td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm208"><mml:mrow><mml:mrow><mml:mn>0.0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm209"><mml:mrow><mml:mrow><mml:mn>0.0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">62&#x02013;55</td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm210"><mml:mrow><mml:mrow><mml:mn>0.0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm211"><mml:mrow><mml:mrow><mml:mn>5.2</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">55&#x02013;48</td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm212"><mml:mrow><mml:mrow><mml:mn>8.5</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm213"><mml:mrow><mml:mrow><mml:mn>28.1</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">48&#x02013;41</td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm214"><mml:mrow><mml:mrow><mml:mn>50.0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm215"><mml:mrow><mml:mrow><mml:mn>82.2</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">41&#x02013;34</td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm216"><mml:mrow><mml:mrow><mml:mn>87.3</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm217"><mml:mrow><mml:mrow><mml:mn>94.7</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">34&#x02013;27</td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm218"><mml:mrow><mml:mrow><mml:mn>100.0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm219"><mml:mrow><mml:mrow><mml:mn>100.0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">27&#x02013;20</td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm220"><mml:mrow><mml:mrow><mml:mn>100.0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm221"><mml:mrow><mml:mrow><mml:mn>100.0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0003c;20</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm222"><mml:mrow><mml:mrow><mml:mn>100.0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm223"><mml:mrow><mml:mrow><mml:mn>100.0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr></tbody></table></table-wrap></floats-group></article>